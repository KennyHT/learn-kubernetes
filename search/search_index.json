{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Learn Kubernetes Labs \u00b6 These are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.","title":"Home"},{"location":"#welcome-to-the-learn-kubernetes-labs","text":"These are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.","title":"Welcome to the Learn Kubernetes Labs"},{"location":"deployments/","text":"Deployments \u00b6 Create the deployment object: 1 kubectl apply -f manifests/deployment.yaml Get the state of the deploloyment: 1 2 kubectl get deployments kubectl describe deployment kuard-deployment Observing the change to our Pods \u00b6 In a termial window, observe the state change to the list of pods. We're going to keep this around for this entire lab. 1 kubectl get pods --watch Scale our pods using Deployments \u00b6 Update spec.replicas in deployment.yaml to be 5 , then deploy our changes: 1 kubectl apply -f manifests/deployment.yaml Notice the old pods don't get deleted straight away. This is because of spec.minReadySeconds value of 30 seconds. Update spec.replicas in deployment.yaml to be 20 , then deploy our changes. Did all of the Pods come up? Let's go checkout the Kubernetes dashboard. Update spec.replicas in deployment.yaml to be 2 again. Deploy a new version of the kuard container \u00b6 Update your deployment.yaml file, changing the image tag to 2 and adding the following under spec.template.metadata in order to provide a reason for the manifest change. This is something that would be template driven by your CD system. 1 2 annotations : kubernetes . io / change - cause : 'Upgraded to version 2.' Then apply your change: 1 kubectl apply -f manifests/deployment.yaml Observe the deployment history: 1 kubectl rollout history deployment/kuard-deployment Debugging Tip: Isolating a pod or pods from a ReplicaSet \u00b6 If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong. To do this, you can change or overwrite the labels (depending upon the replicaset label selector). Let's take one of the deployment pods out of service. 1 2 3 4 5 6 7 8 # Get a pod name kubectl get pods # Take it out of service by changing its label # NOTE: Changing the label value is only to disassociate it the replicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value. kubectl label pods kuard-deployment--<ID> --overwrite app=kuard-quarantined. This will cause Kubernetes to disassociate that pod with the deployment which in turn, will cause Kubernetes to create a new pod. As the renamed pod is now unmanaged, you can exec into it without fear of it being killed. Cleanup \u00b6 We can easily cleanup all of the resources in these labs by telling Kubernetes to delete all of the resources found in the manifests directory. 1 kubectl delete -f manifests/ It's ok that some of these were not found. Hope you enjoyed these labs!","title":"Deployments"},{"location":"deployments/#deployments","text":"Create the deployment object: 1 kubectl apply -f manifests/deployment.yaml Get the state of the deploloyment: 1 2 kubectl get deployments kubectl describe deployment kuard-deployment","title":"Deployments"},{"location":"deployments/#observing-the-change-to-our-pods","text":"In a termial window, observe the state change to the list of pods. We're going to keep this around for this entire lab. 1 kubectl get pods --watch","title":"Observing the change to our Pods"},{"location":"deployments/#scale-our-pods-using-deployments","text":"Update spec.replicas in deployment.yaml to be 5 , then deploy our changes: 1 kubectl apply -f manifests/deployment.yaml Notice the old pods don't get deleted straight away. This is because of spec.minReadySeconds value of 30 seconds. Update spec.replicas in deployment.yaml to be 20 , then deploy our changes. Did all of the Pods come up? Let's go checkout the Kubernetes dashboard. Update spec.replicas in deployment.yaml to be 2 again.","title":"Scale our pods using Deployments"},{"location":"deployments/#deploy-a-new-version-of-the-kuard-container","text":"Update your deployment.yaml file, changing the image tag to 2 and adding the following under spec.template.metadata in order to provide a reason for the manifest change. This is something that would be template driven by your CD system. 1 2 annotations : kubernetes . io / change - cause : 'Upgraded to version 2.' Then apply your change: 1 kubectl apply -f manifests/deployment.yaml Observe the deployment history: 1 kubectl rollout history deployment/kuard-deployment","title":"Deploy a new version of the kuard container"},{"location":"deployments/#debugging-tip-isolating-a-pod-or-pods-from-a-replicaset","text":"If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong. To do this, you can change or overwrite the labels (depending upon the replicaset label selector). Let's take one of the deployment pods out of service. 1 2 3 4 5 6 7 8 # Get a pod name kubectl get pods # Take it out of service by changing its label # NOTE: Changing the label value is only to disassociate it the replicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value. kubectl label pods kuard-deployment--<ID> --overwrite app=kuard-quarantined. This will cause Kubernetes to disassociate that pod with the deployment which in turn, will cause Kubernetes to create a new pod. As the renamed pod is now unmanaged, you can exec into it without fear of it being killed.","title":"Debugging Tip: Isolating a pod or pods from a ReplicaSet"},{"location":"deployments/#cleanup","text":"We can easily cleanup all of the resources in these labs by telling Kubernetes to delete all of the resources found in the manifests directory. 1 kubectl delete -f manifests/ It's ok that some of these were not found. Hope you enjoyed these labs!","title":"Cleanup"},{"location":"namespaces/","text":"Namespaces \u00b6 Namespaces allow you to break a Kubernetes cluster into several virtual clusters. What namespaces exist for your cluster? 1 kubectl get namespaces What objects are in a particular namespace? 1 kubectl get all --namespace=kube-system Let's create the learn-k8s namespace that we'll use at times during this training. 1 kubectl apply -f manifests/namespace.yaml Verify it succeeded. 1 kubectl get namespaces The kubectl context \u00b6 A context is just that, a context for what cluster, which user and which namespace should be used for deploying resources. You can use contexts for switching between different clusters or even the same cluster but with a different user, different namespace or both. We're going to create a context for the docker-for-desktop cluster that sets the namespace to learn-k8s so all of the resources you deploy for these labs don't interfere with any resources you may have in the default namespace. That's what namespaces are for! To keep things separated. We'll call the context learn-k8s (first positional argument). 1 kubectl config set-context learn-k8s --cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s But this has only created the context. Asking for the pods without specifying a namespace lists the pods for the default namespace. 1 kubectl get pods Let's tell kubectl to use our new `learn-k8s context. 1 kubectl config use-context learn-k8s Test that it worked: 1 kubectl get pods If you get an error message like The connection to the server localhost:8080 was refused - did you specify the right host or port? , its probably because the cluster name in your $HOME/.kube/config context you created doesn't match any cluster names in your context as the default server address for kubectl is localhost:8080 . Note You can also switch the context by using the Docker toolbar utility.","title":"Namespaces"},{"location":"namespaces/#namespaces","text":"Namespaces allow you to break a Kubernetes cluster into several virtual clusters. What namespaces exist for your cluster? 1 kubectl get namespaces What objects are in a particular namespace? 1 kubectl get all --namespace=kube-system Let's create the learn-k8s namespace that we'll use at times during this training. 1 kubectl apply -f manifests/namespace.yaml Verify it succeeded. 1 kubectl get namespaces","title":"Namespaces"},{"location":"namespaces/#the-kubectl-context","text":"A context is just that, a context for what cluster, which user and which namespace should be used for deploying resources. You can use contexts for switching between different clusters or even the same cluster but with a different user, different namespace or both. We're going to create a context for the docker-for-desktop cluster that sets the namespace to learn-k8s so all of the resources you deploy for these labs don't interfere with any resources you may have in the default namespace. That's what namespaces are for! To keep things separated. We'll call the context learn-k8s (first positional argument). 1 kubectl config set-context learn-k8s --cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s But this has only created the context. Asking for the pods without specifying a namespace lists the pods for the default namespace. 1 kubectl get pods Let's tell kubectl to use our new `learn-k8s context. 1 kubectl config use-context learn-k8s Test that it worked: 1 kubectl get pods If you get an error message like The connection to the server localhost:8080 was refused - did you specify the right host or port? , its probably because the cluster name in your $HOME/.kube/config context you created doesn't match any cluster names in your context as the default server address for kubectl is localhost:8080 . Note You can also switch the context by using the Docker toolbar utility.","title":"The kubectl context"},{"location":"pods/","text":"Pods Lab \u00b6 In this lab, we will deploy a Pod, the unit of compute to our Kubernetes cluster. You will notice that we always use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same. See the page on Object Management using kubetcl for more info. Introducing the Kubernetes Up and Running Demo (kuard) Pod \u00b6 From the excellent Kubernetes Up and Running book, we're going to use this container to demonstrate many of Kubernetes Pod management features. Deploy our kuard Pod: 1 kubectl apply -f manifests/pod.yaml Verify that the container is running in the Pod and is listening on port 8080: 1 2 kubectl get pods kuard-pod --template=' {{ ( index ( index .spec.containers 0 ) .ports 0 ) .containerPort }}{{ \"\\n\" }} ' # >>> 8080 Pod names \u00b6 Pod names must be unique for a namespace. For fun, let's deploy the kuard-pod into the default namespace. 1 kubectl apply -f manifests/pod.yaml --namespace=default No complaints about the Pod having the same name as its in a different namespace. Let's now delete it as we'll be using the learn-k8s namespace for this training. 1 kubectl delete -f manifests/pod.yaml --namespace=default For development purposes, often you only need a single Pod exposed. If so, then you can just deploy the Pod and port forward from your host. 1 kubectl port-forward kuard-pod 8080:8080 View the API in your browser at http://localhost:8080/ Kill the port-forwarding process in your terminal using a SIGINT (Ctrl+C). Let's inspect our Pod (in a few different ways): 1 2 3 4 5 kubectl get pods kuard-pod kubectl get pods kuard-pod -o wide kubectl describe pods kuard-pod kubectl get pods kuard-pod -o yaml kubectl get pods kuard-pod -o json | jq Docker commands such as logs and exec have Pod equivalents. Let's view the logs for our Pod: 1 kubectl logs kuard-pod This works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name. 1 kubectl logs kuard-pod kuard If we want to tail the logs, we need to supply the -f flag. 1 kubectl logs kuard-pod kuard -f We can see regular requests for /healthy and /ready . This is being called by the kubelet that is responsible for monitoring the health of the pods on each worker node. Accessing a Pod \u00b6 Wouldn't it be great if we could get into a container inside our Pod, as easily as we the docker container exec command. Turns out we can. 1 kubectl exec -it kuard-pod sh By default, kubectl will execute the command against the first container in the spec.containers list. To specify which Pod you want to access, use the --container (or -c ) option along with the container name: 1 kubectl exec -it kuard-pod --container kuard sh Now let's see how Kubernetes reacts to us setting the kuard container to an unhealthy state. 1 kubectl port-forward kuard-pod 8080:8080 Then in another terminal: 1 make event-stream Or... 1 make watch-pods Now let's set go to http://localhost:8080/ and set the health to unhealthy and we'll watch Kubernetes give it a chance to recover, then when it exceeds the unhealthy count threshold, kill the Pod and replace it with another. A debugging Pod \u00b6 Sometimes you may want to inject a Pod into the current namespace for introspection purposes. First, let's get the IP of the kuard Pod. 1 kubectl get pods kuard-pod -o wide Then, with a single command, we can launch a deployment and exec into our debugging container. 1 make debug-container-up Now let's try hitting the Pod directly using its IP address. 1 wget <IP>:8080 -q -O - We're going to use this debug container later when we look at services. For now, let's exit. As a result of exiting, it should also kill the deployment. Verify by running. 1 kubectl get deployments If you can still see your debug container running, then use the following command to delete it. 1 make debug-container-down Finally, remove the kuars Pod: 1 kubectl delete -f manifests/pod.yaml","title":"Pods"},{"location":"pods/#pods-lab","text":"In this lab, we will deploy a Pod, the unit of compute to our Kubernetes cluster. You will notice that we always use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same. See the page on Object Management using kubetcl for more info.","title":"Pods Lab"},{"location":"pods/#introducing-the-kubernetes-up-and-running-demo-kuard-pod","text":"From the excellent Kubernetes Up and Running book, we're going to use this container to demonstrate many of Kubernetes Pod management features. Deploy our kuard Pod: 1 kubectl apply -f manifests/pod.yaml Verify that the container is running in the Pod and is listening on port 8080: 1 2 kubectl get pods kuard-pod --template=' {{ ( index ( index .spec.containers 0 ) .ports 0 ) .containerPort }}{{ \"\\n\" }} ' # >>> 8080","title":"Introducing the Kubernetes Up and Running Demo (kuard) Pod"},{"location":"pods/#pod-names","text":"Pod names must be unique for a namespace. For fun, let's deploy the kuard-pod into the default namespace. 1 kubectl apply -f manifests/pod.yaml --namespace=default No complaints about the Pod having the same name as its in a different namespace. Let's now delete it as we'll be using the learn-k8s namespace for this training. 1 kubectl delete -f manifests/pod.yaml --namespace=default For development purposes, often you only need a single Pod exposed. If so, then you can just deploy the Pod and port forward from your host. 1 kubectl port-forward kuard-pod 8080:8080 View the API in your browser at http://localhost:8080/ Kill the port-forwarding process in your terminal using a SIGINT (Ctrl+C). Let's inspect our Pod (in a few different ways): 1 2 3 4 5 kubectl get pods kuard-pod kubectl get pods kuard-pod -o wide kubectl describe pods kuard-pod kubectl get pods kuard-pod -o yaml kubectl get pods kuard-pod -o json | jq Docker commands such as logs and exec have Pod equivalents. Let's view the logs for our Pod: 1 kubectl logs kuard-pod This works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name. 1 kubectl logs kuard-pod kuard If we want to tail the logs, we need to supply the -f flag. 1 kubectl logs kuard-pod kuard -f We can see regular requests for /healthy and /ready . This is being called by the kubelet that is responsible for monitoring the health of the pods on each worker node.","title":"Pod names"},{"location":"pods/#accessing-a-pod","text":"Wouldn't it be great if we could get into a container inside our Pod, as easily as we the docker container exec command. Turns out we can. 1 kubectl exec -it kuard-pod sh By default, kubectl will execute the command against the first container in the spec.containers list. To specify which Pod you want to access, use the --container (or -c ) option along with the container name: 1 kubectl exec -it kuard-pod --container kuard sh Now let's see how Kubernetes reacts to us setting the kuard container to an unhealthy state. 1 kubectl port-forward kuard-pod 8080:8080 Then in another terminal: 1 make event-stream Or... 1 make watch-pods Now let's set go to http://localhost:8080/ and set the health to unhealthy and we'll watch Kubernetes give it a chance to recover, then when it exceeds the unhealthy count threshold, kill the Pod and replace it with another.","title":"Accessing a Pod"},{"location":"pods/#a-debugging-pod","text":"Sometimes you may want to inject a Pod into the current namespace for introspection purposes. First, let's get the IP of the kuard Pod. 1 kubectl get pods kuard-pod -o wide Then, with a single command, we can launch a deployment and exec into our debugging container. 1 make debug-container-up Now let's try hitting the Pod directly using its IP address. 1 wget <IP>:8080 -q -O - We're going to use this debug container later when we look at services. For now, let's exit. As a result of exiting, it should also kill the deployment. Verify by running. 1 kubectl get deployments If you can still see your debug container running, then use the following command to delete it. 1 make debug-container-down Finally, remove the kuars Pod: 1 kubectl delete -f manifests/pod.yaml","title":"A debugging Pod"},{"location":"rbac/","text":"RBAC \u00b6 Role-Based Access Control controls access to the Kubernetes API and restricts what objects in what namespaces an authenticated user is authorizd to access. RBAC has been GA in Kubernetes since 1.8 (late 2017) so most Kubernetes installs now enable it out of the box which is great! RBAC is a big topic and let's start off bu looking at the three A's - Authorization, Access, and Auditing. Note Authentication and Authorization is a HUGE topic and we can't hope to cover it all in one lab. What we do want to do, is deviate from the defaults that are setup to expose you to the options that you have. This is because when it comes time to access Kubernetes clusters in live environments, your .kube/config file for kubectl won't be setup for you. Authorization \u00b6 Purpose: Is kubectl able to make a request? Using Docker for Desktop as an example, let's take a look at our kubectl config file at $HOME/.kube/config . Taking a look under the users key, we can see that a user called docker-for-desktop using X509 Client Certs as the authentication method. This is not the only game in town, with Tokens and Authentication proxies being others ( more info here ). Step 1. Backup your current config \u00b6 Copy and paste your current $HOME/.kube/config file as $HOME/.kube/config.bak in case you break something. Note If you happen to completely break your kubectl configuration and did not take a back-up, Docker for Desktop has you covered. Just reset your Kubernentes cluster back to its original settings by going to Docker > Preferences/Settings > Reset and click the Reset Kubernetes cluster button. This will also reset your `$HOME/.kube/config, restoring your original certificate authenication config in the process. Step 2. Get the token from the default-token secret. \u00b6 Let's get a service account token by using kubectl to access the .data.token value of the default-token key. 1 2 3 4 5 6 # Extract just the token name DEFAULT_TOKEN_NAME=$(kubectl get secrets --namespace=kube-system | grep default-token | awk '{print $1}') # Get the default token value from the secret and base64 decode it # Note: `KUBE_AUTH_TOKEN` is our name and does not mean anything to `kubectl`. KUBE_AUTH_TOKEN=$(kubectl get secret --namespace=kube-system ${ DEFAULT_TOKEN_NAME } --template='{{.data.token}}' | base64 -D) Now it's time to experiment with our token and talking to the API server. Experiment 1. Authentication via the --token flag \u00b6 We have the option of overriding the default user for our current context by using the --token flag. First, let's observe authentication failing if we pass in an invalid token. 1 kubectl get nodes --token=123 You should've gotten an error like error: You must be logged in to the server (Unauthorized). . Now let's try it out using our legit token. 1 kubectl get nodes --token= ${ KUBE_AUTH_TOKEN } Success! Experiment 2. Hit the Kubernetes API directly (no kubectl) \u00b6 Let's hit the /version and /metrics endpoints for the API Server using our token. We need to know the API server endpoint which we can get from our $HOME/.kube/config and the clusters > cluster > server value. Note We'll be using the --insecure flag because the SSL certificate installed is a self-signed certificate. First, let's make sure it fails without the token. 1 curl --insecure -H \"Authorization: Bearer ABC123\" https://localhost:6443/version And now with the token. 1 2 3 4 5 # Version curl --insecure -H \"Authorization: Bearer ${ KUBE_AUTH_TOKEN } \" https://localhost:6443/version # Metrics curl --insecure -H \"Authorization: Bearer ${ KUBE_AUTH_TOKEN } \" https://localhost:6443/metrics Experiment 3. Let's create a new user for our kubectl config. \u00b6 Now let's create a new user called jack-sparrow and we'll make him belong to a new context . 1 kubectl config set-credentials jack-sparrow --token= ${ DEFAULT_TOKEN } If we want to use our current context but override the current user ( docker-for-desktop ), then we can pass kubectl the --user flag. 1 2 3 4 5 # Fail with non-existent user kubectl get nodes --user black-beard # Success with jack kubectl get nodes --user jack-sparrow We can go one step further and create a new context called pirate that will target the docker-for-desktop node, but using our jack-sparrow user and the learn-k8s namespace. 1 kubectl config set-context pirate -cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s Access \u00b6 Does the authenticated user have the permissions required to access and perform the required task against the reqeusted resource? TODO CONTENT. Auditing \u00b6 Record an audit trail of who did what. TODO CONTENT. Roles and Cluster Roles \u00b6 Binding Roles to Users and Groups \u00b6","title":"RBAC"},{"location":"rbac/#rbac","text":"Role-Based Access Control controls access to the Kubernetes API and restricts what objects in what namespaces an authenticated user is authorizd to access. RBAC has been GA in Kubernetes since 1.8 (late 2017) so most Kubernetes installs now enable it out of the box which is great! RBAC is a big topic and let's start off bu looking at the three A's - Authorization, Access, and Auditing. Note Authentication and Authorization is a HUGE topic and we can't hope to cover it all in one lab. What we do want to do, is deviate from the defaults that are setup to expose you to the options that you have. This is because when it comes time to access Kubernetes clusters in live environments, your .kube/config file for kubectl won't be setup for you.","title":"RBAC"},{"location":"rbac/#authorization","text":"Purpose: Is kubectl able to make a request? Using Docker for Desktop as an example, let's take a look at our kubectl config file at $HOME/.kube/config . Taking a look under the users key, we can see that a user called docker-for-desktop using X509 Client Certs as the authentication method. This is not the only game in town, with Tokens and Authentication proxies being others ( more info here ).","title":"Authorization"},{"location":"rbac/#step-1-backup-your-current-config","text":"Copy and paste your current $HOME/.kube/config file as $HOME/.kube/config.bak in case you break something. Note If you happen to completely break your kubectl configuration and did not take a back-up, Docker for Desktop has you covered. Just reset your Kubernentes cluster back to its original settings by going to Docker > Preferences/Settings > Reset and click the Reset Kubernetes cluster button. This will also reset your `$HOME/.kube/config, restoring your original certificate authenication config in the process.","title":"Step 1. Backup your current config"},{"location":"rbac/#step-2-get-the-token-from-the-default-token-secret","text":"Let's get a service account token by using kubectl to access the .data.token value of the default-token key. 1 2 3 4 5 6 # Extract just the token name DEFAULT_TOKEN_NAME=$(kubectl get secrets --namespace=kube-system | grep default-token | awk '{print $1}') # Get the default token value from the secret and base64 decode it # Note: `KUBE_AUTH_TOKEN` is our name and does not mean anything to `kubectl`. KUBE_AUTH_TOKEN=$(kubectl get secret --namespace=kube-system ${ DEFAULT_TOKEN_NAME } --template='{{.data.token}}' | base64 -D) Now it's time to experiment with our token and talking to the API server.","title":"Step 2. Get the token from the default-token secret."},{"location":"rbac/#experiment-1-authentication-via-the-token-flag","text":"We have the option of overriding the default user for our current context by using the --token flag. First, let's observe authentication failing if we pass in an invalid token. 1 kubectl get nodes --token=123 You should've gotten an error like error: You must be logged in to the server (Unauthorized). . Now let's try it out using our legit token. 1 kubectl get nodes --token= ${ KUBE_AUTH_TOKEN } Success!","title":"Experiment 1. Authentication via the --token flag"},{"location":"rbac/#experiment-2-hit-the-kubernetes-api-directly-no-kubectl","text":"Let's hit the /version and /metrics endpoints for the API Server using our token. We need to know the API server endpoint which we can get from our $HOME/.kube/config and the clusters > cluster > server value. Note We'll be using the --insecure flag because the SSL certificate installed is a self-signed certificate. First, let's make sure it fails without the token. 1 curl --insecure -H \"Authorization: Bearer ABC123\" https://localhost:6443/version And now with the token. 1 2 3 4 5 # Version curl --insecure -H \"Authorization: Bearer ${ KUBE_AUTH_TOKEN } \" https://localhost:6443/version # Metrics curl --insecure -H \"Authorization: Bearer ${ KUBE_AUTH_TOKEN } \" https://localhost:6443/metrics","title":"Experiment 2. Hit the Kubernetes API directly (no kubectl)"},{"location":"rbac/#experiment-3-lets-create-a-new-user-for-our-kubectl-config","text":"Now let's create a new user called jack-sparrow and we'll make him belong to a new context . 1 kubectl config set-credentials jack-sparrow --token= ${ DEFAULT_TOKEN } If we want to use our current context but override the current user ( docker-for-desktop ), then we can pass kubectl the --user flag. 1 2 3 4 5 # Fail with non-existent user kubectl get nodes --user black-beard # Success with jack kubectl get nodes --user jack-sparrow We can go one step further and create a new context called pirate that will target the docker-for-desktop node, but using our jack-sparrow user and the learn-k8s namespace. 1 kubectl config set-context pirate -cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s","title":"Experiment 3. Let's create a new user for our kubectl config."},{"location":"rbac/#access","text":"Does the authenticated user have the permissions required to access and perform the required task against the reqeusted resource? TODO CONTENT.","title":"Access"},{"location":"rbac/#auditing","text":"Record an audit trail of who did what. TODO CONTENT.","title":"Auditing"},{"location":"rbac/#roles-and-cluster-roles","text":"","title":"Roles and Cluster Roles"},{"location":"rbac/#binding-roles-to-users-and-groups","text":"","title":"Binding Roles to Users and Groups"},{"location":"replicasets/","text":"ReplicaSets Lab \u00b6 This lab is a bit different in that you'll be creating the the replicaset.yaml yourself. Code challenge \u00b6 Using the documentation at https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in template/replicaset.yaml that uses the information from the manifests/pod.yaml file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :). For the solution, see the manifests/replicaset-solution.yaml . We will use the solution with a copy of the manifests/replicaset-solution.yaml renamed to manifests/replicaset.yaml below but use your own replicaset.yam file if you completed the code challenge. To deploy: 1 kubectl apply -f manifests/replicaset.yaml To verify: 1 2 kubectl get replicasets kubectl get pods Notice that the name of the pods is different for those that were created by the ReplicaSet. Notice too that the kuard-service now has multiple endpoints. 1 kubectl describe services kuard-service Replicasets are in that they give us a way to manage multiple Pods, but they have no ability to help us change between one version of Pods to another. That's what deployments are for. Clean up now by deleting your replicaset: 1 kubectl delete -f manifests/replicaset.yaml","title":"ReplicaSets"},{"location":"replicasets/#replicasets-lab","text":"This lab is a bit different in that you'll be creating the the replicaset.yaml yourself.","title":"ReplicaSets Lab"},{"location":"replicasets/#code-challenge","text":"Using the documentation at https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in template/replicaset.yaml that uses the information from the manifests/pod.yaml file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :). For the solution, see the manifests/replicaset-solution.yaml . We will use the solution with a copy of the manifests/replicaset-solution.yaml renamed to manifests/replicaset.yaml below but use your own replicaset.yam file if you completed the code challenge. To deploy: 1 kubectl apply -f manifests/replicaset.yaml To verify: 1 2 kubectl get replicasets kubectl get pods Notice that the name of the pods is different for those that were created by the ReplicaSet. Notice too that the kuard-service now has multiple endpoints. 1 kubectl describe services kuard-service Replicasets are in that they give us a way to manage multiple Pods, but they have no ability to help us change between one version of Pods to another. That's what deployments are for. Clean up now by deleting your replicaset: 1 kubectl delete -f manifests/replicaset.yaml","title":"Code challenge"},{"location":"services/","text":"Services Lab \u00b6 To illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it. Note Because Docker for Desktop does not bind the NodePort associated with the LoadBalancer Service type to localhost (on the host), I will be using the a type of NodePort service. Currently when using type LoadBalancer , every port value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports. Create the service: 1 kubectl apply -f manifests/service.yaml Inspect the service so we can get the randomly assigned ports: 1 kubectl describe service kuard-service | grep Port Confirm we don't have any pods matching the label the service is querying for: 1 kubectl get pods kuard-pod We can tell the service doesn't have Pods because there are no endpoints associated with the service. 1 kubectl describe service kuard-service Let's create the kuard Pod again. 1 kubectl apply -f manifests/pod.yaml And now, we can see one endpoint. 1 kubectl describe service kuard-service Service name by DNS \u00b6 This must be done through a container running in the same namespace as the service. Now that we've deployed the kuard Pod, we can get the fully qualified domain name (FQDN). 1 2 make debug-container-up nslookup kuard-service Note The output from nslookup which says \"nslookup: can't resolve '(null)': Name does not resolve\" is expected because there is no DNS server to perform the lookup against. Now let's try making a request to the service. 1 wget kuard-service -q -O - What's neat is that the service is constantly monitoring the Pods who's labels match it's selector query and so knows which Pod Ip addresses (endpoints) to route the request to. Exit (to kill) the debug container. The ClusterIP \u00b6 When a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info. Virtual doesn't mean magic through. Let's find the Cluster IP value of the kuard-service . The Cluster IP is guaranteed not to change. 1 echo `kubectl get service kuard-service --template=' {{ .spec.clusterIP }} '` Let's verify that this is a legit IP address by hitting it from the Docker VM. 1 2 make docker-vm-shell wget <IP> -q -O - No magic and no hidden IP address scheme. Kubernetes is awesome! Exit out of the Docker VM shell","title":"Services"},{"location":"services/#services-lab","text":"To illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it. Note Because Docker for Desktop does not bind the NodePort associated with the LoadBalancer Service type to localhost (on the host), I will be using the a type of NodePort service. Currently when using type LoadBalancer , every port value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports. Create the service: 1 kubectl apply -f manifests/service.yaml Inspect the service so we can get the randomly assigned ports: 1 kubectl describe service kuard-service | grep Port Confirm we don't have any pods matching the label the service is querying for: 1 kubectl get pods kuard-pod We can tell the service doesn't have Pods because there are no endpoints associated with the service. 1 kubectl describe service kuard-service Let's create the kuard Pod again. 1 kubectl apply -f manifests/pod.yaml And now, we can see one endpoint. 1 kubectl describe service kuard-service","title":"Services Lab"},{"location":"services/#service-name-by-dns","text":"This must be done through a container running in the same namespace as the service. Now that we've deployed the kuard Pod, we can get the fully qualified domain name (FQDN). 1 2 make debug-container-up nslookup kuard-service Note The output from nslookup which says \"nslookup: can't resolve '(null)': Name does not resolve\" is expected because there is no DNS server to perform the lookup against. Now let's try making a request to the service. 1 wget kuard-service -q -O - What's neat is that the service is constantly monitoring the Pods who's labels match it's selector query and so knows which Pod Ip addresses (endpoints) to route the request to. Exit (to kill) the debug container.","title":"Service name by DNS"},{"location":"services/#the-clusterip","text":"When a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info. Virtual doesn't mean magic through. Let's find the Cluster IP value of the kuard-service . The Cluster IP is guaranteed not to change. 1 echo `kubectl get service kuard-service --template=' {{ .spec.clusterIP }} '` Let's verify that this is a legit IP address by hitting it from the Docker VM. 1 2 make docker-vm-shell wget <IP> -q -O - No magic and no hidden IP address scheme. Kubernetes is awesome! Exit out of the Docker VM shell","title":"The ClusterIP"},{"location":"setup/","text":"Development Environment Setup \u00b6 These instructions are currently for MacOS users with homebrew installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the Windows Subsystem for Linux . Windows users are encouraged to install the Windows Subsystem for Linux . Docker for Desktop \u00b6 Download and install Docker for Desktop . If you're on Linux, then this guide using kubeadm by Kubernetes super star Liz Rice (even though she uses the VM from her Mac) should work like a charm. Kubernetes Check \u00b6 Execute the following commands to check all is well. 1 2 kubectl cluster-info kubectl get nodes Kubectl Proxy \u00b6 For easy access to services running in your development environment, you can use kubectl proxy. 1 kubectl proxy You can now access services throgh the tunnel exposed on localhost port 8001. See this page from the Kubernetes docs for learning how to manually construct links to your services. Required images \u00b6 To save some time during the class, you can run make setup to pull down the two required images for these labs. 1 make setup System utilities \u00b6 watch \u00b6 The watch binary allows you to observe the output from running a command every n seconds. While kubernetes has a built-in --watch flag, I often don't use it as it doesnt't flush the previous output. To install it with homebrew on the Mac, use brew install watch . jq \u00b6 The jq binary allows us to nicely format, search and extract data from JSON on the commandline. For a lot of Kubernetes commands, we'll be using the --template flag which uses golang template syntax but jq is in general, a very useful tool, even if all you use it for is to pretty print JSON. To install it with homebrew on the Mac, use brew install watch . Bash completion for kubectl \u00b6 The kubectl CLI is huge and the completion functionality will help save time and help you learn. For this to work, you need bash-completion installed by homebrew by running: 1 brew install bash-completion Then add this to your ~/.bash_profile: 1 [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion Then install the completion for kubectl. 1 kubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl","title":"Setup"},{"location":"setup/#development-environment-setup","text":"These instructions are currently for MacOS users with homebrew installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the Windows Subsystem for Linux . Windows users are encouraged to install the Windows Subsystem for Linux .","title":"Development Environment Setup"},{"location":"setup/#docker-for-desktop","text":"Download and install Docker for Desktop . If you're on Linux, then this guide using kubeadm by Kubernetes super star Liz Rice (even though she uses the VM from her Mac) should work like a charm.","title":"Docker for Desktop"},{"location":"setup/#kubernetes-check","text":"Execute the following commands to check all is well. 1 2 kubectl cluster-info kubectl get nodes","title":"Kubernetes Check"},{"location":"setup/#kubectl-proxy","text":"For easy access to services running in your development environment, you can use kubectl proxy. 1 kubectl proxy You can now access services throgh the tunnel exposed on localhost port 8001. See this page from the Kubernetes docs for learning how to manually construct links to your services.","title":"Kubectl Proxy"},{"location":"setup/#required-images","text":"To save some time during the class, you can run make setup to pull down the two required images for these labs. 1 make setup","title":"Required images"},{"location":"setup/#system-utilities","text":"","title":"System utilities"},{"location":"setup/#watch","text":"The watch binary allows you to observe the output from running a command every n seconds. While kubernetes has a built-in --watch flag, I often don't use it as it doesnt't flush the previous output. To install it with homebrew on the Mac, use brew install watch .","title":"watch"},{"location":"setup/#jq","text":"The jq binary allows us to nicely format, search and extract data from JSON on the commandline. For a lot of Kubernetes commands, we'll be using the --template flag which uses golang template syntax but jq is in general, a very useful tool, even if all you use it for is to pretty print JSON. To install it with homebrew on the Mac, use brew install watch .","title":"jq"},{"location":"setup/#bash-completion-for-kubectl","text":"The kubectl CLI is huge and the completion functionality will help save time and help you learn. For this to work, you need bash-completion installed by homebrew by running: 1 brew install bash-completion Then add this to your ~/.bash_profile: 1 [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion Then install the completion for kubectl. 1 kubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl","title":"Bash completion for kubectl"}]}