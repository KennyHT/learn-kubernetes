{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Learn Kubernetes Labs\n\u00b6\n\n\nThese are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-learn-kubernetes-labs",
            "text": "These are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.",
            "title": "Welcome to the Learn Kubernetes Labs"
        },
        {
            "location": "/0-setup/",
            "text": "Development Environment Setup\n\u00b6\n\n\nThese instructions are currently for MacOS users with \nhomebrew\n installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the \nWindows Subsystem for Linux\n.\n\n\nWindows users are encouraged to install the \nWindows Subsystem for Linux\n.\n\n\nDocker for Desktop\n\u00b6\n\n\nDownload and install Docker for Desktop.\n\n\nKubectl Proxy\n\u00b6\n\n\nFor easy of \n\n\nNow you can access the dashboard at \nhttps://localhost:30000/\n.\n\n\nIf you're on a Mac or Windows using the WSL, you can use the bash code at this \nGitHub Gist\n to give you two functions for starting and stopping the Kubernetes Dashboard.\n\n\nRequired images\n\u00b6\n\n\nThese labs depends on images that are built using the \nhttps://github.com/ryan-blunden/learn-docker\n repository.\n\n\nTo build the required images:\n\n\n1\nmake build\n\n\n\n\n\n\nThe watch command\n\u00b6\n\n\nThe watch command allows you to observe the output from running a command every n seconds.\n\n\nTo install it with homebrew, use \nbrew install watch\n.\n\n\nBash completion for kubectl\n\u00b6\n\n\nThe kubectl CLI is huge and the completion functionality will help save time and help you learn.\n\n\nFor this to work, you need \nbash-completion\n installed by homebrew by running:\n\n\n1\nbrew install bash-completion\n\n\n\n\n\n\nThen add this to your ~/.bash_profile:\n\n\n1\n[ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion\n\n\n\n\n\n\nThen install the completion for kubectl.\n\n\n1\nkubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl",
            "title": "Setup"
        },
        {
            "location": "/0-setup/#development-environment-setup",
            "text": "These instructions are currently for MacOS users with  homebrew  installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the  Windows Subsystem for Linux .  Windows users are encouraged to install the  Windows Subsystem for Linux .",
            "title": "Development Environment Setup"
        },
        {
            "location": "/0-setup/#docker-for-desktop",
            "text": "Download and install Docker for Desktop.",
            "title": "Docker for Desktop"
        },
        {
            "location": "/0-setup/#kubectl-proxy",
            "text": "For easy of   Now you can access the dashboard at  https://localhost:30000/ .  If you're on a Mac or Windows using the WSL, you can use the bash code at this  GitHub Gist  to give you two functions for starting and stopping the Kubernetes Dashboard.",
            "title": "Kubectl Proxy"
        },
        {
            "location": "/0-setup/#required-images",
            "text": "These labs depends on images that are built using the  https://github.com/ryan-blunden/learn-docker  repository.  To build the required images:  1 make build",
            "title": "Required images"
        },
        {
            "location": "/0-setup/#the-watch-command",
            "text": "The watch command allows you to observe the output from running a command every n seconds.  To install it with homebrew, use  brew install watch .",
            "title": "The watch command"
        },
        {
            "location": "/0-setup/#bash-completion-for-kubectl",
            "text": "The kubectl CLI is huge and the completion functionality will help save time and help you learn.  For this to work, you need  bash-completion  installed by homebrew by running:  1 brew install bash-completion   Then add this to your ~/.bash_profile:  1 [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion   Then install the completion for kubectl.  1 kubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl",
            "title": "Bash completion for kubectl"
        },
        {
            "location": "/1-namespaces/",
            "text": "Namespaces\n\u00b6\n\n\nNamespaces allow you to break a Kubernetes cluster into several virtual clusters.\n\n\nWhat namespaces exist for your cluster?\n\n\n1\nkubectl get namespaces\n\n\n\n\n\n\nWhat objects are in a particular namespace?\n\n\n1\nkubectl get all --namespace=kube-system\n\n\n\n\n\n\nLet's create the \nlearn-k8s\n namespace that we'll use at times during this training.\n\n\n1\nkubectl apply -f objects/namespace.yaml\n\n\n\n\n\n\nVerify it succeeded.\n\n\n1\nkubectl get namespaces",
            "title": "Namespaces"
        },
        {
            "location": "/1-namespaces/#namespaces",
            "text": "Namespaces allow you to break a Kubernetes cluster into several virtual clusters.  What namespaces exist for your cluster?  1 kubectl get namespaces   What objects are in a particular namespace?  1 kubectl get all --namespace=kube-system   Let's create the  learn-k8s  namespace that we'll use at times during this training.  1 kubectl apply -f objects/namespace.yaml   Verify it succeeded.  1 kubectl get namespaces",
            "title": "Namespaces"
        },
        {
            "location": "/2-pods/",
            "text": "Pods Lab\n\u00b6\n\n\nIn this lab, we will deploy a Pod, the unit of compute to our Kubernetes instance.\n\n\nYou will notice that we \nalways\n use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same.\n\n\nSee the page on \nObject Management using kubetcl\n for more info.\n\n\nIntroducing the Kubernetes Up and Running Demo (kuard) Pod\n\u00b6\n\n\nFrom the excellent \nKubernetes Up and Running\n book, we're going to use this handy container to help demonstrate many of Kubernetes Pod management features.\n\n\nDeploy our kuard Pod:\n\n\n1\nkubectl apply -f objects/pod.yaml\n\n\n\n\n\n\nVerify that the container is running in the Pod and is listening on port 8080:\n\n\n1\n2\nkubectl get pods kuard-pod --template='\n{{\n(\nindex\n \n(\nindex\n \n.spec.containers\n \n0\n)\n.ports\n \n0\n)\n.containerPort\n}}{{\n\"\\n\"\n}}\n'\n\n\n# >>> 8080\n\n\n\n\n\n\n\nFor fun, let's deploy the \nkuard-pod\n into the \nlearn-k8s\n namespace.\n\n\n1\nkubectl apply -f objects/pod.yaml --namespace=learn-k8s\n\n\n\n\n\n\nNo complaints about the Pod having the same name as its in a different namespace. \n\n\nLet's now delete it as we'll be using the default namespace for the training for ease of use.\n\n\n1\nkubectl delete -f objects/pod.yaml --namespace=learn-k8s\n\n\n\n\n\n\nFor development purposes, often you only need a single Pod exposed. If so, then you can just deploy the Pod and port forward from your host.\n\n\n1\nkubectl port-forward kuard-pod 8080:8080\n\n\n\n\n\n\nView the API in your browser at http://localhost:8080/\n\n\nKill the port-forwarding process in your terminal using a SIGINT (Ctrl+c).\n\n\nLet's inspect our Pod (in a few different ways):\n\n\n1\n2\n3\n4\n5\nkubectl get pods kuard-pod\nkubectl get pods kuard-pod -o wide\nkubectl describe pods kuard-pod\nkubectl get pods kuard-pod -o yaml\nkubectl get pods kuard-pod -o json | jq\n\n\n\n\n\n\nDocker commands such as \nlogs\n and \nexec\n have Pod equivalents.\n\n\nLet's view the logs for our Pod:\n\n\n1\nkubectl logs kuard-pod\n\n\n\n\n\n\nThis works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name.\n\n\n1\nkubectl logs kuard-pod kuard\n\n\n\n\n\n\nIf we want to tail the logs, we need to supply the \n-f\n flag.\n\n\n1\nkubectl logs kuard-pod kuard -f\n\n\n\n\n\n\nWouldn't it be great if we could get into a container inside our Pod, as easily as we the \ndocker container exec\n command. Turns out we can.\n\n\n1\nkubectl exec -it kuard-pod sh\n\n\n\n\n\n\nBy default, \nkubectl\n will execute the command against the first container in the \nspec.containers\n list. \n\n\nTo specify which Pod you want to access, use the \n--container\n (or \n-c\n) option along with the container name:\n\n\n1\nkubectl exec -it pod --container kuard sh\n\n\n\n\n\n\nNow let's see how Kubernetes reacts to us setting the kuard container to an unhealthy state.\n\n\n1\nkubectl port-forward kuard-pod 8080:8080\n\n\n\n\n\n\nThen in another terminal:\n\n\n1\nmake event-stream\n\n\n\n\n\n\nOr...\n\n\n1\nmake watch pods\n\n\n\n\n\n\nNow let's set go to http://localhost:8080/ and set the health to unhealthy and we'll watch Kubernetes give it a chance to recover, then when it exceeds the unhealthy count threshold, kill the Pod and replace it with another. \n\n\nFinally, remove your Pod:\n\n\n1\nkubectl delete -f objects/pod.yaml\n\n\n\n\n\n\nTODO\n\u00b6\n\n\n\n\nShow debug Pod example\n\n\nShow what happens when you change certain set only fields (ports) vs labels or names.\n\n\nAdd horizontal Pod auto-scaling example - https://kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/\n\n\nExample of assigning Pods to a specific node.",
            "title": "Pods"
        },
        {
            "location": "/2-pods/#pods-lab",
            "text": "In this lab, we will deploy a Pod, the unit of compute to our Kubernetes instance.  You will notice that we  always  use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same.  See the page on  Object Management using kubetcl  for more info.",
            "title": "Pods Lab"
        },
        {
            "location": "/2-pods/#introducing-the-kubernetes-up-and-running-demo-kuard-pod",
            "text": "From the excellent  Kubernetes Up and Running  book, we're going to use this handy container to help demonstrate many of Kubernetes Pod management features.  Deploy our kuard Pod:  1 kubectl apply -f objects/pod.yaml   Verify that the container is running in the Pod and is listening on port 8080:  1\n2 kubectl get pods kuard-pod --template=' {{ ( index   ( index   .spec.containers   0 ) .ports   0 ) .containerPort }}{{ \"\\n\" }} '  # >>> 8080    For fun, let's deploy the  kuard-pod  into the  learn-k8s  namespace.  1 kubectl apply -f objects/pod.yaml --namespace=learn-k8s   No complaints about the Pod having the same name as its in a different namespace.   Let's now delete it as we'll be using the default namespace for the training for ease of use.  1 kubectl delete -f objects/pod.yaml --namespace=learn-k8s   For development purposes, often you only need a single Pod exposed. If so, then you can just deploy the Pod and port forward from your host.  1 kubectl port-forward kuard-pod 8080:8080   View the API in your browser at http://localhost:8080/  Kill the port-forwarding process in your terminal using a SIGINT (Ctrl+c).  Let's inspect our Pod (in a few different ways):  1\n2\n3\n4\n5 kubectl get pods kuard-pod\nkubectl get pods kuard-pod -o wide\nkubectl describe pods kuard-pod\nkubectl get pods kuard-pod -o yaml\nkubectl get pods kuard-pod -o json | jq   Docker commands such as  logs  and  exec  have Pod equivalents.  Let's view the logs for our Pod:  1 kubectl logs kuard-pod   This works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name.  1 kubectl logs kuard-pod kuard   If we want to tail the logs, we need to supply the  -f  flag.  1 kubectl logs kuard-pod kuard -f   Wouldn't it be great if we could get into a container inside our Pod, as easily as we the  docker container exec  command. Turns out we can.  1 kubectl exec -it kuard-pod sh   By default,  kubectl  will execute the command against the first container in the  spec.containers  list.   To specify which Pod you want to access, use the  --container  (or  -c ) option along with the container name:  1 kubectl exec -it pod --container kuard sh   Now let's see how Kubernetes reacts to us setting the kuard container to an unhealthy state.  1 kubectl port-forward kuard-pod 8080:8080   Then in another terminal:  1 make event-stream   Or...  1 make watch pods   Now let's set go to http://localhost:8080/ and set the health to unhealthy and we'll watch Kubernetes give it a chance to recover, then when it exceeds the unhealthy count threshold, kill the Pod and replace it with another.   Finally, remove your Pod:  1 kubectl delete -f objects/pod.yaml",
            "title": "Introducing the Kubernetes Up and Running Demo (kuard) Pod"
        },
        {
            "location": "/2-pods/#todo",
            "text": "Show debug Pod example  Show what happens when you change certain set only fields (ports) vs labels or names.  Add horizontal Pod auto-scaling example - https://kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/  Example of assigning Pods to a specific node.",
            "title": "TODO"
        },
        {
            "location": "/3-services/",
            "text": "Services Lab\n\u00b6\n\n\nTo illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it.\n\n\n\n\nNote\n\n\nBecause Docker for Desktop does not bind the \nNodePort\n associated with the \nLoadBalancer\n Service type to localhost (on the host), I will be using the a type of \nNodePort\n service.\n\n\nCurrently when using type \nLoadBalancer\n, every \nport\n value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports.\n\n\n\n\nCreate the service:\n\n\n1\nkubectl apply -f objects/service.yaml\n\n\n\n\n\n\nInspect the service so we can get the randomly assigned ports:\n\n\n1\nkubectl describe service kuard-service | grep Port\n\n\n\n\n\n\nConfirm we don't have any pods matching the label the service is querying for:\n\n\n1\nkubectl get pods kuard-pod\n\n\n\n\n\n\nThen open a new terminal window and create the kuard pod.\n\n\n1\nkubectl apply -f objects/pod.yaml\n\n\n\n\n\n\nService name by DNS\n\u00b6\n\n\nThis must be done through a container running in the same namespace as the service.\n\n\nNow that we've deployed our API pod, we can get the FQDN.\n\n\n1\n2\ndebug-container-up\nnslookup kuard-service\n\n\n\n\n\n\nThe ClusterIP\n\u00b6\n\n\nWhen a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info.\n\n\nLet's find the \nCluster IP\n value of the \nkuard-service\n. The Cluster IP is guaranteed not to change.\n\n\n1\necho `kubectl get service kuard-service --template='\n{{\n.spec.clusterIP\n}}\n'`\n\n\n\n\n\n\n\nLet's verify that this is a legit IP address by hitting it from the Docker VM.\n\n\n1\n2\n3\nkubectl describe service kuard-service\nmake docker-vm-shell\nping <ip address>",
            "title": "Services"
        },
        {
            "location": "/3-services/#services-lab",
            "text": "To illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it.   Note  Because Docker for Desktop does not bind the  NodePort  associated with the  LoadBalancer  Service type to localhost (on the host), I will be using the a type of  NodePort  service.  Currently when using type  LoadBalancer , every  port  value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports.   Create the service:  1 kubectl apply -f objects/service.yaml   Inspect the service so we can get the randomly assigned ports:  1 kubectl describe service kuard-service | grep Port   Confirm we don't have any pods matching the label the service is querying for:  1 kubectl get pods kuard-pod   Then open a new terminal window and create the kuard pod.  1 kubectl apply -f objects/pod.yaml",
            "title": "Services Lab"
        },
        {
            "location": "/3-services/#service-name-by-dns",
            "text": "This must be done through a container running in the same namespace as the service.  Now that we've deployed our API pod, we can get the FQDN.  1\n2 debug-container-up\nnslookup kuard-service",
            "title": "Service name by DNS"
        },
        {
            "location": "/3-services/#the-clusterip",
            "text": "When a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info.  Let's find the  Cluster IP  value of the  kuard-service . The Cluster IP is guaranteed not to change.  1 echo `kubectl get service kuard-service --template=' {{ .spec.clusterIP }} '`    Let's verify that this is a legit IP address by hitting it from the Docker VM.  1\n2\n3 kubectl describe service kuard-service\nmake docker-vm-shell\nping <ip address>",
            "title": "The ClusterIP"
        },
        {
            "location": "/4-replicas/",
            "text": "Replicas Lab\n\u00b6\n\n\nThis lab is a bit different in that you'll be creating the the \napi-replicaset.yaml\n yourself.\n\n\nCode challenge\n\u00b6\n\n\nUsing the documentation at \nhttps://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\n, create a replicaset resource in \ntemplate/replicaset.yaml\n that uses the information from the \nobjects/pod.yaml\n file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :).\n\n\nFor the solution, see the \nobjects/api-replicaset-solution.yaml\n.\n\n\nTo deploy:\n\n\n1\nkubectl apply -f objects/api-replicaset.yaml\n\n\n\n\n\n\nTo verify:\n\n\n1\n2\nkubectl get replicasets\nkubectl get pods\n\n\n\n\n\n\nNotice that the name of the pods is different for those that were created by the ReplicaSet.\n\n\n1\n2\n# TODO: Fix. Not working.\n\n\nkubectl get pod api-replicaset-<HASH> --template='\n{{\n(\nindex\n \n(\nindex\n \n.metadata.annotations\n))\n}}{{\n\"\\n\"\n}}\n'\n\n\n\n\n\n\n\nClean up now by deleting your replicaset:\n\n\n1\nkubectl delete -f objects/api-replicaset.yaml",
            "title": "Replicas"
        },
        {
            "location": "/4-replicas/#replicas-lab",
            "text": "This lab is a bit different in that you'll be creating the the  api-replicaset.yaml  yourself.",
            "title": "Replicas Lab"
        },
        {
            "location": "/4-replicas/#code-challenge",
            "text": "Using the documentation at  https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in  template/replicaset.yaml  that uses the information from the  objects/pod.yaml  file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :).  For the solution, see the  objects/api-replicaset-solution.yaml .  To deploy:  1 kubectl apply -f objects/api-replicaset.yaml   To verify:  1\n2 kubectl get replicasets\nkubectl get pods   Notice that the name of the pods is different for those that were created by the ReplicaSet.  1\n2 # TODO: Fix. Not working.  kubectl get pod api-replicaset-<HASH> --template=' {{ ( index   ( index   .metadata.annotations )) }}{{ \"\\n\" }} '    Clean up now by deleting your replicaset:  1 kubectl delete -f objects/api-replicaset.yaml",
            "title": "Code challenge"
        },
        {
            "location": "/5-deployments/",
            "text": "Deployments\n\u00b6\n\n\nCreate the deployment object:\n\n\n1\nkubectl apply -f objects/api-deployments.yaml\n\n\n\n\n\n\nGet the state of the deploloyment:\n\n\n1\n2\nkubectl get deployments\nkubectl describe deployment api-deployment\n\n\n\n\n\n\nScale our pods using Deployments\n\u00b6\n\n\nUpdate \nspec.replicas\n in \napi-deployments.yaml\n to be 15, then deploy our changes:\n\n\n1\nkubectl apply -f objects/api-deployments.yaml\n\n\n\n\n\n\nThen observe the state change to the list of pods every second using the \nwatch\n command:\n\n\n1\nwatch -n 1 kubectl get pods\n\n\n\n\n\n\nNotice the old pods don't get deleted straight away. This is because of \nspec.minReadySeconds\n value of 30 seconds.\n\n\nDeploy a new version of our API container\n\u00b6\n\n\nLet's change the data in \napi/data/users/index.json\n to change the data served by our API.\n\n\nThen lets build a new version of the api container:\n\n\n1\nmake api-build VERSION=2.0\n\n\n\n\n\n\nNow update your \napi-deployments.yaml\n file, changing the image tag to \n2.0\n and adding the following under \nspec.template.metadata\n in order to provide a reason for the manifest change. This is something that would be template driven by your CD system.\n\n\n1\n2\nannotations\n:\n\n    \nkubernetes\n.\nio\n/\n \nchange\n-\ncause\n:\n \n'Data changed in API, updating to 2.0'\n\n\n\n\n\n\n\nThen apply your change:\n\n\n1\nkubectl apply -f objects/api-deployments.yaml\n\n\n\n\n\n\nThen observe the state change to the list of pods every second using the \nwatch\n command:\n\n\n1\nwatch -n 1 kubectl get pods\n\n\n\n\n\n\nObserve the deployment history:\n\n\n1\nkubectl rollout history deployment/api-deployment\n\n\n\n\n\n\nDebugging Tip: Isolating a pod or pods from a ReplicaSet\n\u00b6\n\n\nIf a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong.\n\n\nTo do this, you can change or overwrite the labels (depending upon the replicaset label selector).\n\n\nLet's take one of the deployment pods out of service.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Get a pod name\nkubectl get pods\n\n# Take it out of service by changing its label\n# NOTE: Changing the label value is only to disassociate it the\nreplicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value.\n\nkubectl label pods api-replicaset-<HASH> --overwrite app=api-quarantined.\n\n\n\n\n\n\nThis will cause Kubernetes to disassociate that pod with the replicaset which in turn, will cause Kubernetes to create a new pod. As this pod is now unmanaged, you can exec into it without fear of it being killed.\n\n\nTODO\n: Add name change as well to make it more obvious which pod was disassociated with the replicaset.",
            "title": "Deployments"
        },
        {
            "location": "/5-deployments/#deployments",
            "text": "Create the deployment object:  1 kubectl apply -f objects/api-deployments.yaml   Get the state of the deploloyment:  1\n2 kubectl get deployments\nkubectl describe deployment api-deployment",
            "title": "Deployments"
        },
        {
            "location": "/5-deployments/#scale-our-pods-using-deployments",
            "text": "Update  spec.replicas  in  api-deployments.yaml  to be 15, then deploy our changes:  1 kubectl apply -f objects/api-deployments.yaml   Then observe the state change to the list of pods every second using the  watch  command:  1 watch -n 1 kubectl get pods   Notice the old pods don't get deleted straight away. This is because of  spec.minReadySeconds  value of 30 seconds.",
            "title": "Scale our pods using Deployments"
        },
        {
            "location": "/5-deployments/#deploy-a-new-version-of-our-api-container",
            "text": "Let's change the data in  api/data/users/index.json  to change the data served by our API.  Then lets build a new version of the api container:  1 make api-build VERSION=2.0   Now update your  api-deployments.yaml  file, changing the image tag to  2.0  and adding the following under  spec.template.metadata  in order to provide a reason for the manifest change. This is something that would be template driven by your CD system.  1\n2 annotations : \n     kubernetes . io /   change - cause :   'Data changed in API, updating to 2.0'    Then apply your change:  1 kubectl apply -f objects/api-deployments.yaml   Then observe the state change to the list of pods every second using the  watch  command:  1 watch -n 1 kubectl get pods   Observe the deployment history:  1 kubectl rollout history deployment/api-deployment",
            "title": "Deploy a new version of our API container"
        },
        {
            "location": "/5-deployments/#debugging-tip-isolating-a-pod-or-pods-from-a-replicaset",
            "text": "If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong.  To do this, you can change or overwrite the labels (depending upon the replicaset label selector).  Let's take one of the deployment pods out of service.  1\n2\n3\n4\n5\n6\n7\n8 # Get a pod name\nkubectl get pods\n\n# Take it out of service by changing its label\n# NOTE: Changing the label value is only to disassociate it the\nreplicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value.\n\nkubectl label pods api-replicaset-<HASH> --overwrite app=api-quarantined.   This will cause Kubernetes to disassociate that pod with the replicaset which in turn, will cause Kubernetes to create a new pod. As this pod is now unmanaged, you can exec into it without fear of it being killed.  TODO : Add name change as well to make it more obvious which pod was disassociated with the replicaset.",
            "title": "Debugging Tip: Isolating a pod or pods from a ReplicaSet"
        }
    ]
}