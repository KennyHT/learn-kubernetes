{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Learn Kubernetes Labs\n\u00b6\n\n\nThese are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-learn-kubernetes-labs",
            "text": "These are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.",
            "title": "Welcome to the Learn Kubernetes Labs"
        },
        {
            "location": "/setup/",
            "text": "Development Environment Setup\n\u00b6\n\n\nThese instructions are currently for MacOS users with \nhomebrew\n installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the \nWindows Subsystem for Linux\n.\n\n\nWindows users are encouraged to install the \nWindows Subsystem for Linux\n.\n\n\nDocker for Desktop\n\u00b6\n\n\nDownload and install \nDocker for Desktop\n.\n\n\nIf you're on Linux, \nthen this guide using kubeadm\n by Kubernetes super star Liz Rice (even though she uses the VM from her Mac) should work like a charm.\n\n\nKubectl Proxy\n\u00b6\n\n\nFor easy access to services running in your development environment, you can use kubectl proxy.\n\n\n1\nkubectl proxy\n\n\n\n\n\n\nYou can now access services throgh the tunnel exposed on localhost port 8001.\n\n\nSee \nthis page from the Kubernetes docs\n for learning how to manually construct links to your services.\n\n\nRequired images\n\u00b6\n\n\nTo save some time during the class, you can run \nmake setup\n to pull down the two required images for these labs.\n\n\n1\nmake setup\n\n\n\n\n\n\nSystem utilities\n\u00b6\n\n\nwatch\n\u00b6\n\n\nThe \nwatch\n binary allows you to observe the output from running a command every n seconds.\n\n\nWhile kubernetes has a built-in \n--watch\n flag, I often don't use it as it doesnt't flush the previous output.\n\n\nTo install it with homebrew on the Mac, use \nbrew install watch\n.\n\n\njq\n\u00b6\n\n\nThe \njq\n binary allows us to nicely format, search and extract data from JSON on the commandline. \n\n\nFor a lot of Kubernetes commands, we'll be using the \n--template\n flag which uses \ngolang\n template syntax but \njq\n is in general, a very useful tool to have lying around, even if you use it to do nothing more than just pretty print JSON in the terminal.\n\n\nTo install it with homebrew on the Mac, use \nbrew install watch\n. \n\n\nBash completion for kubectl\n\u00b6\n\n\nThe kubectl CLI is huge and the completion functionality will help save time and help you learn.\n\n\nFor this to work, you need \nbash-completion\n installed by homebrew by running:\n\n\n1\nbrew install bash-completion\n\n\n\n\n\n\nThen add this to your ~/.bash_profile:\n\n\n1\n[ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion\n\n\n\n\n\n\nThen install the completion for kubectl.\n\n\n1\nkubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl",
            "title": "Setup"
        },
        {
            "location": "/setup/#development-environment-setup",
            "text": "These instructions are currently for MacOS users with  homebrew  installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the  Windows Subsystem for Linux .  Windows users are encouraged to install the  Windows Subsystem for Linux .",
            "title": "Development Environment Setup"
        },
        {
            "location": "/setup/#docker-for-desktop",
            "text": "Download and install  Docker for Desktop .  If you're on Linux,  then this guide using kubeadm  by Kubernetes super star Liz Rice (even though she uses the VM from her Mac) should work like a charm.",
            "title": "Docker for Desktop"
        },
        {
            "location": "/setup/#kubectl-proxy",
            "text": "For easy access to services running in your development environment, you can use kubectl proxy.  1 kubectl proxy   You can now access services throgh the tunnel exposed on localhost port 8001.  See  this page from the Kubernetes docs  for learning how to manually construct links to your services.",
            "title": "Kubectl Proxy"
        },
        {
            "location": "/setup/#required-images",
            "text": "To save some time during the class, you can run  make setup  to pull down the two required images for these labs.  1 make setup",
            "title": "Required images"
        },
        {
            "location": "/setup/#system-utilities",
            "text": "",
            "title": "System utilities"
        },
        {
            "location": "/setup/#watch",
            "text": "The  watch  binary allows you to observe the output from running a command every n seconds.  While kubernetes has a built-in  --watch  flag, I often don't use it as it doesnt't flush the previous output.  To install it with homebrew on the Mac, use  brew install watch .",
            "title": "watch"
        },
        {
            "location": "/setup/#jq",
            "text": "The  jq  binary allows us to nicely format, search and extract data from JSON on the commandline.   For a lot of Kubernetes commands, we'll be using the  --template  flag which uses  golang  template syntax but  jq  is in general, a very useful tool to have lying around, even if you use it to do nothing more than just pretty print JSON in the terminal.  To install it with homebrew on the Mac, use  brew install watch .",
            "title": "jq"
        },
        {
            "location": "/setup/#bash-completion-for-kubectl",
            "text": "The kubectl CLI is huge and the completion functionality will help save time and help you learn.  For this to work, you need  bash-completion  installed by homebrew by running:  1 brew install bash-completion   Then add this to your ~/.bash_profile:  1 [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion   Then install the completion for kubectl.  1 kubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl",
            "title": "Bash completion for kubectl"
        },
        {
            "location": "/namespaces/",
            "text": "Namespaces\n\u00b6\n\n\nNamespaces allow you to break a Kubernetes cluster into several virtual clusters.\n\n\nWhat namespaces exist for your cluster?\n\n\n1\nkubectl get namespaces\n\n\n\n\n\n\nWhat objects are in a particular namespace?\n\n\n1\nkubectl get all --namespace=kube-system\n\n\n\n\n\n\nLet's create the \nlearn-k8s\n namespace that we'll use at times during this training.\n\n\n1\nkubectl apply -f objects/namespace.yaml\n\n\n\n\n\n\nVerify it succeeded.\n\n\n1\nkubectl get namespaces\n\n\n\n\n\n\nThe kubectl context\n\u00b6\n\n\nA \ncontext\n is just that, a context for what cluster, which user and which namespace should be used for communicating to an API server.\n\n\nYou can use contexts for switching between different clusters or even the same cluster but with a different user, different namespace or both.\n\n\nWe're going to create a context for the \ndocker-for-desktop\n cluster that sets the namespace to \nlearn-k8s\n so all of the resources you deploy for these labs don't interfere with any resources you may have in the \ndefault\n namespace. That's what namespaces are for! To keep things separated.\n\n\nWe'll call the context \nlearn-k8s\n (first positional argument).\n\n\n1\nkubectl config set-context learn-k8s --cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s\n\n\n\n\n\n\nThen we weed to tell \nkubectl\n to use this \ncontext\n.\n\n\n1\nkubectl config use-context learn-k8s\n\n\n\n\n\n\nTest that it worked:\n\n\n1\nkubectl get pods\n\n\n\n\n\n\nIf you get an error message like \nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n, its probably because the cluster name in your \n$HOME/.kube/config\n context you created doesn't match any cluster names in your context as the default server address for \nkubectl\n is \nlocalhost:8080\n.\n\n\n\n\nNote\n\n\nYou can also switch the context by using the Docker toolbar utility.",
            "title": "Namespaces"
        },
        {
            "location": "/namespaces/#namespaces",
            "text": "Namespaces allow you to break a Kubernetes cluster into several virtual clusters.  What namespaces exist for your cluster?  1 kubectl get namespaces   What objects are in a particular namespace?  1 kubectl get all --namespace=kube-system   Let's create the  learn-k8s  namespace that we'll use at times during this training.  1 kubectl apply -f objects/namespace.yaml   Verify it succeeded.  1 kubectl get namespaces",
            "title": "Namespaces"
        },
        {
            "location": "/namespaces/#the-kubectl-context",
            "text": "A  context  is just that, a context for what cluster, which user and which namespace should be used for communicating to an API server.  You can use contexts for switching between different clusters or even the same cluster but with a different user, different namespace or both.  We're going to create a context for the  docker-for-desktop  cluster that sets the namespace to  learn-k8s  so all of the resources you deploy for these labs don't interfere with any resources you may have in the  default  namespace. That's what namespaces are for! To keep things separated.  We'll call the context  learn-k8s  (first positional argument).  1 kubectl config set-context learn-k8s --cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s   Then we weed to tell  kubectl  to use this  context .  1 kubectl config use-context learn-k8s   Test that it worked:  1 kubectl get pods   If you get an error message like  The connection to the server localhost:8080 was refused - did you specify the right host or port? , its probably because the cluster name in your  $HOME/.kube/config  context you created doesn't match any cluster names in your context as the default server address for  kubectl  is  localhost:8080 .   Note  You can also switch the context by using the Docker toolbar utility.",
            "title": "The kubectl context"
        },
        {
            "location": "/pods/",
            "text": "Pods Lab\n\u00b6\n\n\nIn this lab, we will deploy a Pod, the unit of compute to our Kubernetes instance.\n\n\nYou will notice that we \nalways\n use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same.\n\n\nSee the page on \nObject Management using kubetcl\n for more info.\n\n\nIntroducing the Kubernetes Up and Running Demo (kuard) Pod\n\u00b6\n\n\nFrom the excellent \nKubernetes Up and Running\n book, we're going to use this handy container to help demonstrate many of Kubernetes Pod management features.\n\n\nDeploy our kuard Pod:\n\n\n1\nkubectl apply -f objects/pod.yaml\n\n\n\n\n\n\nVerify that the container is running in the Pod and is listening on port 8080:\n\n\n1\n2\nkubectl get pods kuard-pod --template='\n{{\n(\nindex\n \n(\nindex\n \n.spec.containers\n \n0\n)\n.ports\n \n0\n)\n.containerPort\n}}{{\n\"\\n\"\n}}\n'\n\n\n# >>> 8080\n\n\n\n\n\n\n\nFor fun, let's deploy the \nkuard-pod\n into the \nlearn-k8s\n namespace.\n\n\n1\nkubectl apply -f objects/pod.yaml --namespace=learn-k8s\n\n\n\n\n\n\nNo complaints about the Pod having the same name as its in a different namespace. \n\n\nLet's now delete it as we'll be using the default namespace for the training for ease of use.\n\n\n1\nkubectl delete -f objects/pod.yaml --namespace=learn-k8s\n\n\n\n\n\n\nFor development purposes, often you only need a single Pod exposed. If so, then you can just deploy the Pod and port forward from your host.\n\n\n1\nkubectl port-forward kuard-pod 8080:8080\n\n\n\n\n\n\nView the API in your browser at http://localhost:8080/\n\n\nKill the port-forwarding process in your terminal using a SIGINT (Ctrl+c).\n\n\nLet's inspect our Pod (in a few different ways):\n\n\n1\n2\n3\n4\n5\nkubectl get pods kuard-pod\nkubectl get pods kuard-pod -o wide\nkubectl describe pods kuard-pod\nkubectl get pods kuard-pod -o yaml\nkubectl get pods kuard-pod -o json | jq\n\n\n\n\n\n\nDocker commands such as \nlogs\n and \nexec\n have Pod equivalents.\n\n\nLet's view the logs for our Pod:\n\n\n1\nkubectl logs kuard-pod\n\n\n\n\n\n\nThis works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name.\n\n\n1\nkubectl logs kuard-pod kuard\n\n\n\n\n\n\nIf we want to tail the logs, we need to supply the \n-f\n flag.\n\n\n1\nkubectl logs kuard-pod kuard -f\n\n\n\n\n\n\nWouldn't it be great if we could get into a container inside our Pod, as easily as we the \ndocker container exec\n command. Turns out we can.\n\n\n1\nkubectl exec -it kuard-pod sh\n\n\n\n\n\n\nBy default, \nkubectl\n will execute the command against the first container in the \nspec.containers\n list. \n\n\nTo specify which Pod you want to access, use the \n--container\n (or \n-c\n) option along with the container name:\n\n\n1\nkubectl exec -it kuard-pod --container kuard sh\n\n\n\n\n\n\nNow let's see how Kubernetes reacts to us setting the kuard container to an unhealthy state.\n\n\n1\nkubectl port-forward kuard-pod 8080:8080\n\n\n\n\n\n\nThen in another terminal:\n\n\n1\nmake event-stream\n\n\n\n\n\n\nOr...\n\n\n1\nmake watch-pods\n\n\n\n\n\n\nNow let's set go to http://localhost:8080/ and set the health to unhealthy and we'll watch Kubernetes give it a chance to recover, then when it exceeds the unhealthy count threshold, kill the Pod and replace it with another. \n\n\nFinally, remove your Pod:\n\n\n1\nkubectl delete -f objects/pod.yaml",
            "title": "Pods"
        },
        {
            "location": "/pods/#pods-lab",
            "text": "In this lab, we will deploy a Pod, the unit of compute to our Kubernetes instance.  You will notice that we  always  use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same.  See the page on  Object Management using kubetcl  for more info.",
            "title": "Pods Lab"
        },
        {
            "location": "/pods/#introducing-the-kubernetes-up-and-running-demo-kuard-pod",
            "text": "From the excellent  Kubernetes Up and Running  book, we're going to use this handy container to help demonstrate many of Kubernetes Pod management features.  Deploy our kuard Pod:  1 kubectl apply -f objects/pod.yaml   Verify that the container is running in the Pod and is listening on port 8080:  1\n2 kubectl get pods kuard-pod --template=' {{ ( index   ( index   .spec.containers   0 ) .ports   0 ) .containerPort }}{{ \"\\n\" }} '  # >>> 8080    For fun, let's deploy the  kuard-pod  into the  learn-k8s  namespace.  1 kubectl apply -f objects/pod.yaml --namespace=learn-k8s   No complaints about the Pod having the same name as its in a different namespace.   Let's now delete it as we'll be using the default namespace for the training for ease of use.  1 kubectl delete -f objects/pod.yaml --namespace=learn-k8s   For development purposes, often you only need a single Pod exposed. If so, then you can just deploy the Pod and port forward from your host.  1 kubectl port-forward kuard-pod 8080:8080   View the API in your browser at http://localhost:8080/  Kill the port-forwarding process in your terminal using a SIGINT (Ctrl+c).  Let's inspect our Pod (in a few different ways):  1\n2\n3\n4\n5 kubectl get pods kuard-pod\nkubectl get pods kuard-pod -o wide\nkubectl describe pods kuard-pod\nkubectl get pods kuard-pod -o yaml\nkubectl get pods kuard-pod -o json | jq   Docker commands such as  logs  and  exec  have Pod equivalents.  Let's view the logs for our Pod:  1 kubectl logs kuard-pod   This works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name.  1 kubectl logs kuard-pod kuard   If we want to tail the logs, we need to supply the  -f  flag.  1 kubectl logs kuard-pod kuard -f   Wouldn't it be great if we could get into a container inside our Pod, as easily as we the  docker container exec  command. Turns out we can.  1 kubectl exec -it kuard-pod sh   By default,  kubectl  will execute the command against the first container in the  spec.containers  list.   To specify which Pod you want to access, use the  --container  (or  -c ) option along with the container name:  1 kubectl exec -it kuard-pod --container kuard sh   Now let's see how Kubernetes reacts to us setting the kuard container to an unhealthy state.  1 kubectl port-forward kuard-pod 8080:8080   Then in another terminal:  1 make event-stream   Or...  1 make watch-pods   Now let's set go to http://localhost:8080/ and set the health to unhealthy and we'll watch Kubernetes give it a chance to recover, then when it exceeds the unhealthy count threshold, kill the Pod and replace it with another.   Finally, remove your Pod:  1 kubectl delete -f objects/pod.yaml",
            "title": "Introducing the Kubernetes Up and Running Demo (kuard) Pod"
        },
        {
            "location": "/services/",
            "text": "Services Lab\n\u00b6\n\n\nTo illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it.\n\n\n\n\nNote\n\n\nBecause Docker for Desktop does not bind the \nNodePort\n associated with the \nLoadBalancer\n Service type to localhost (on the host), I will be using the a type of \nNodePort\n service.\n\n\nCurrently when using type \nLoadBalancer\n, every \nport\n value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports.\n\n\n\n\nCreate the service:\n\n\n1\nkubectl apply -f objects/service.yaml\n\n\n\n\n\n\nInspect the service so we can get the randomly assigned ports:\n\n\n1\nkubectl describe service kuard-service | grep Port\n\n\n\n\n\n\nConfirm we don't have any pods matching the label the service is querying for:\n\n\n1\nkubectl get pods kuard-pod\n\n\n\n\n\n\nThen open a new terminal window and create the kuard pod.\n\n\n1\nkubectl apply -f objects/pod.yaml\n\n\n\n\n\n\nService name by DNS\n\u00b6\n\n\nThis must be done through a container running in the same namespace as the service.\n\n\nNow that we've deployed our API pod, we can get the FQDN.\n\n\n1\n2\ndebug-container-up\nnslookup kuard-service\n\n\n\n\n\n\nThe ClusterIP\n\u00b6\n\n\nWhen a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info.\n\n\nLet's find the \nCluster IP\n value of the \nkuard-service\n. The Cluster IP is guaranteed not to change.\n\n\n1\necho `kubectl get service kuard-service --template='\n{{\n.spec.clusterIP\n}}\n'`\n\n\n\n\n\n\n\nLet's verify that this is a legit IP address by hitting it from the Docker VM.\n\n\n1\n2\n3\nkubectl describe service kuard-service\nmake docker-vm-shell\nping <ip address>",
            "title": "Services"
        },
        {
            "location": "/services/#services-lab",
            "text": "To illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it.   Note  Because Docker for Desktop does not bind the  NodePort  associated with the  LoadBalancer  Service type to localhost (on the host), I will be using the a type of  NodePort  service.  Currently when using type  LoadBalancer , every  port  value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports.   Create the service:  1 kubectl apply -f objects/service.yaml   Inspect the service so we can get the randomly assigned ports:  1 kubectl describe service kuard-service | grep Port   Confirm we don't have any pods matching the label the service is querying for:  1 kubectl get pods kuard-pod   Then open a new terminal window and create the kuard pod.  1 kubectl apply -f objects/pod.yaml",
            "title": "Services Lab"
        },
        {
            "location": "/services/#service-name-by-dns",
            "text": "This must be done through a container running in the same namespace as the service.  Now that we've deployed our API pod, we can get the FQDN.  1\n2 debug-container-up\nnslookup kuard-service",
            "title": "Service name by DNS"
        },
        {
            "location": "/services/#the-clusterip",
            "text": "When a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info.  Let's find the  Cluster IP  value of the  kuard-service . The Cluster IP is guaranteed not to change.  1 echo `kubectl get service kuard-service --template=' {{ .spec.clusterIP }} '`    Let's verify that this is a legit IP address by hitting it from the Docker VM.  1\n2\n3 kubectl describe service kuard-service\nmake docker-vm-shell\nping <ip address>",
            "title": "The ClusterIP"
        },
        {
            "location": "/replicas/",
            "text": "Replicas Lab\n\u00b6\n\n\nThis lab is a bit different in that you'll be creating the the \napi-replicaset.yaml\n yourself.\n\n\nCode challenge\n\u00b6\n\n\nUsing the documentation at \nhttps://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\n, create a replicaset resource in \ntemplate/replicaset.yaml\n that uses the information from the \nobjects/pod.yaml\n file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :).\n\n\nFor the solution, see the \nobjects/api-replicaset-solution.yaml\n.\n\n\nTo deploy:\n\n\n1\nkubectl apply -f objects/api-replicaset.yaml\n\n\n\n\n\n\nTo verify:\n\n\n1\n2\nkubectl get replicasets\nkubectl get pods\n\n\n\n\n\n\nNotice that the name of the pods is different for those that were created by the ReplicaSet.\n\n\n1\n2\n# TODO: Fix. Not working.\n\n\nkubectl get pod api-replicaset-<HASH> --template='\n{{\n(\nindex\n \n(\nindex\n \n.metadata.annotations\n))\n}}{{\n\"\\n\"\n}}\n'\n\n\n\n\n\n\n\nClean up now by deleting your replicaset:\n\n\n1\nkubectl delete -f objects/api-replicaset.yaml",
            "title": "Replicas"
        },
        {
            "location": "/replicas/#replicas-lab",
            "text": "This lab is a bit different in that you'll be creating the the  api-replicaset.yaml  yourself.",
            "title": "Replicas Lab"
        },
        {
            "location": "/replicas/#code-challenge",
            "text": "Using the documentation at  https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in  template/replicaset.yaml  that uses the information from the  objects/pod.yaml  file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :).  For the solution, see the  objects/api-replicaset-solution.yaml .  To deploy:  1 kubectl apply -f objects/api-replicaset.yaml   To verify:  1\n2 kubectl get replicasets\nkubectl get pods   Notice that the name of the pods is different for those that were created by the ReplicaSet.  1\n2 # TODO: Fix. Not working.  kubectl get pod api-replicaset-<HASH> --template=' {{ ( index   ( index   .metadata.annotations )) }}{{ \"\\n\" }} '    Clean up now by deleting your replicaset:  1 kubectl delete -f objects/api-replicaset.yaml",
            "title": "Code challenge"
        },
        {
            "location": "/deployments/",
            "text": "Deployments\n\u00b6\n\n\nCreate the deployment object:\n\n\n1\nkubectl apply -f objects/api-deployments.yaml\n\n\n\n\n\n\nGet the state of the deploloyment:\n\n\n1\n2\nkubectl get deployments\nkubectl describe deployment api-deployment\n\n\n\n\n\n\nScale our pods using Deployments\n\u00b6\n\n\nUpdate \nspec.replicas\n in \napi-deployments.yaml\n to be 15, then deploy our changes:\n\n\n1\nkubectl apply -f objects/api-deployments.yaml\n\n\n\n\n\n\nThen observe the state change to the list of pods every second using the \nwatch\n command:\n\n\n1\nwatch -n 1 kubectl get pods\n\n\n\n\n\n\nNotice the old pods don't get deleted straight away. This is because of \nspec.minReadySeconds\n value of 30 seconds.\n\n\nDeploy a new version of our API container\n\u00b6\n\n\nLet's change the data in \napi/data/users/index.json\n to change the data served by our API.\n\n\nThen lets build a new version of the api container:\n\n\n1\nmake api-build VERSION=2.0\n\n\n\n\n\n\nNow update your \napi-deployments.yaml\n file, changing the image tag to \n2.0\n and adding the following under \nspec.template.metadata\n in order to provide a reason for the manifest change. This is something that would be template driven by your CD system.\n\n\n1\n2\nannotations\n:\n\n    \nkubernetes\n.\nio\n/\n \nchange\n-\ncause\n:\n \n'Data changed in API, updating to 2.0'\n\n\n\n\n\n\n\nThen apply your change:\n\n\n1\nkubectl apply -f objects/api-deployments.yaml\n\n\n\n\n\n\nThen observe the state change to the list of pods every second using the \nwatch\n command:\n\n\n1\nwatch -n 1 kubectl get pods\n\n\n\n\n\n\nObserve the deployment history:\n\n\n1\nkubectl rollout history deployment/api-deployment\n\n\n\n\n\n\nDebugging Tip: Isolating a pod or pods from a ReplicaSet\n\u00b6\n\n\nIf a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong.\n\n\nTo do this, you can change or overwrite the labels (depending upon the replicaset label selector).\n\n\nLet's take one of the deployment pods out of service.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Get a pod name\nkubectl get pods\n\n# Take it out of service by changing its label\n# NOTE: Changing the label value is only to disassociate it the\nreplicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value.\n\nkubectl label pods api-replicaset-<HASH> --overwrite app=api-quarantined.\n\n\n\n\n\n\nThis will cause Kubernetes to disassociate that pod with the replicaset which in turn, will cause Kubernetes to create a new pod. As this pod is now unmanaged, you can exec into it without fear of it being killed.",
            "title": "Deployments"
        },
        {
            "location": "/deployments/#deployments",
            "text": "Create the deployment object:  1 kubectl apply -f objects/api-deployments.yaml   Get the state of the deploloyment:  1\n2 kubectl get deployments\nkubectl describe deployment api-deployment",
            "title": "Deployments"
        },
        {
            "location": "/deployments/#scale-our-pods-using-deployments",
            "text": "Update  spec.replicas  in  api-deployments.yaml  to be 15, then deploy our changes:  1 kubectl apply -f objects/api-deployments.yaml   Then observe the state change to the list of pods every second using the  watch  command:  1 watch -n 1 kubectl get pods   Notice the old pods don't get deleted straight away. This is because of  spec.minReadySeconds  value of 30 seconds.",
            "title": "Scale our pods using Deployments"
        },
        {
            "location": "/deployments/#deploy-a-new-version-of-our-api-container",
            "text": "Let's change the data in  api/data/users/index.json  to change the data served by our API.  Then lets build a new version of the api container:  1 make api-build VERSION=2.0   Now update your  api-deployments.yaml  file, changing the image tag to  2.0  and adding the following under  spec.template.metadata  in order to provide a reason for the manifest change. This is something that would be template driven by your CD system.  1\n2 annotations : \n     kubernetes . io /   change - cause :   'Data changed in API, updating to 2.0'    Then apply your change:  1 kubectl apply -f objects/api-deployments.yaml   Then observe the state change to the list of pods every second using the  watch  command:  1 watch -n 1 kubectl get pods   Observe the deployment history:  1 kubectl rollout history deployment/api-deployment",
            "title": "Deploy a new version of our API container"
        },
        {
            "location": "/deployments/#debugging-tip-isolating-a-pod-or-pods-from-a-replicaset",
            "text": "If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong.  To do this, you can change or overwrite the labels (depending upon the replicaset label selector).  Let's take one of the deployment pods out of service.  1\n2\n3\n4\n5\n6\n7\n8 # Get a pod name\nkubectl get pods\n\n# Take it out of service by changing its label\n# NOTE: Changing the label value is only to disassociate it the\nreplicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value.\n\nkubectl label pods api-replicaset-<HASH> --overwrite app=api-quarantined.   This will cause Kubernetes to disassociate that pod with the replicaset which in turn, will cause Kubernetes to create a new pod. As this pod is now unmanaged, you can exec into it without fear of it being killed.",
            "title": "Debugging Tip: Isolating a pod or pods from a ReplicaSet"
        }
    ]
}