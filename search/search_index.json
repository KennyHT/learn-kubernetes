{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Learn Kubernetes Labs \u00b6 These labs are for the Kubernetes for Developers course taught by Ryan Blunden.","title":"Home"},{"location":"#welcome-to-the-learn-kubernetes-labs","text":"These labs are for the Kubernetes for Developers course taught by Ryan Blunden.","title":"Welcome to the Learn Kubernetes Labs"},{"location":"configmaps/","text":"ConfigMaps \u00b6 ConfigMaps can be created from directories, files, or literal values. This lab is going to focus on the most common use of ConfigMaps which is literal values mapping to environment variables. See the Configure a Pod to Use a ConfigMap page from the Kubernetes docs to see other examples. Anatomy of a ConfigMap \u00b6 A ConfigMap is a collection of key-value pairs, known as its data-source. To illustrate this, let's create a ConfigMap from literal values. 1 kubectl create configmap db-config --from-literal=HOST=db.acmecorp.com --from-literal=NAME=users Now let's get the ConfigMap contents. 1 kubectl get configmap db-config -o yaml You can see each key-value pair passed in on its own line in the data section. Remove the ConfigMap before continuing. 1 kubectl delete configmap db-config Creating ConfigMaps from manifest files \u00b6 We can also create a ConfigMap the declarative way. 1 2 3 kubectl apply -f manifests/configmap-db.yaml kubectl get configmaps kubectl get configmap db-config -o yaml Typically, ConfigMaps will be created as manifest files and if the values need to change, they will either be changed in source or use variable substitution at deploy time in your CI/CD system of choice. How you create the manifest file though does not matter. Mapping ConfigMap keys to Pod environment files \u00b6 This is the most common way to use ConfigMap data. We can map keys to Pod environment variables one by one, or all at once. Let's look at manifests/pod-configmap.yaml for examples of both types. Pods needs a valid ConfigMap to exist ahead of time \u00b6 A Pod that depends on a ConfigMap will fail to start if the ConfigMap does not exist, or if the pod references a ConfigMap key that does not exist. Let's delete all Pods and ConfigMaps from the learn-k8s namespace. 1 kubectl delete pods,configmaps --all The manifests/pod-configmap.yaml file relies on two ConfigMaps. Let's only deploy one ConfigMap. 1 2 3 kubectl apply -f manifests/configmap-db.yaml kubectl apply -f manifests/pod-configmap.yaml kubectl get pod kuard-pod-configmap -o wide We can see the Pod has a status of CreateContainerConfigError . Let's fix this by creating the missing ConfigMap. 1 2 kubectl apply -f manifests/configmap-app.yaml watch kubectl get pod kuard-pod-configmap -o wide After an amount of time, the Pod is able to start and eventually becomes healthy. What's important to remember is that with ConfigMaps (and Secrets), order of operations matter. Also be careful that any Pod spec configuration changes are matched with ConfigMap changes. Verifying our Pod has the expected environment variables \u00b6 1 kubectl exec -it kuard-pod printenv","title":"ConfigMaps"},{"location":"configmaps/#configmaps","text":"ConfigMaps can be created from directories, files, or literal values. This lab is going to focus on the most common use of ConfigMaps which is literal values mapping to environment variables. See the Configure a Pod to Use a ConfigMap page from the Kubernetes docs to see other examples.","title":"ConfigMaps"},{"location":"configmaps/#anatomy-of-a-configmap","text":"A ConfigMap is a collection of key-value pairs, known as its data-source. To illustrate this, let's create a ConfigMap from literal values. 1 kubectl create configmap db-config --from-literal=HOST=db.acmecorp.com --from-literal=NAME=users Now let's get the ConfigMap contents. 1 kubectl get configmap db-config -o yaml You can see each key-value pair passed in on its own line in the data section. Remove the ConfigMap before continuing. 1 kubectl delete configmap db-config","title":"Anatomy of a ConfigMap"},{"location":"configmaps/#creating-configmaps-from-manifest-files","text":"We can also create a ConfigMap the declarative way. 1 2 3 kubectl apply -f manifests/configmap-db.yaml kubectl get configmaps kubectl get configmap db-config -o yaml Typically, ConfigMaps will be created as manifest files and if the values need to change, they will either be changed in source or use variable substitution at deploy time in your CI/CD system of choice. How you create the manifest file though does not matter.","title":"Creating ConfigMaps from manifest files"},{"location":"configmaps/#mapping-configmap-keys-to-pod-environment-files","text":"This is the most common way to use ConfigMap data. We can map keys to Pod environment variables one by one, or all at once. Let's look at manifests/pod-configmap.yaml for examples of both types.","title":"Mapping ConfigMap keys to Pod environment files"},{"location":"configmaps/#pods-needs-a-valid-configmap-to-exist-ahead-of-time","text":"A Pod that depends on a ConfigMap will fail to start if the ConfigMap does not exist, or if the pod references a ConfigMap key that does not exist. Let's delete all Pods and ConfigMaps from the learn-k8s namespace. 1 kubectl delete pods,configmaps --all The manifests/pod-configmap.yaml file relies on two ConfigMaps. Let's only deploy one ConfigMap. 1 2 3 kubectl apply -f manifests/configmap-db.yaml kubectl apply -f manifests/pod-configmap.yaml kubectl get pod kuard-pod-configmap -o wide We can see the Pod has a status of CreateContainerConfigError . Let's fix this by creating the missing ConfigMap. 1 2 kubectl apply -f manifests/configmap-app.yaml watch kubectl get pod kuard-pod-configmap -o wide After an amount of time, the Pod is able to start and eventually becomes healthy. What's important to remember is that with ConfigMaps (and Secrets), order of operations matter. Also be careful that any Pod spec configuration changes are matched with ConfigMap changes.","title":"Pods needs a valid ConfigMap to exist ahead of time"},{"location":"configmaps/#verifying-our-pod-has-the-expected-environment-variables","text":"1 kubectl exec -it kuard-pod printenv","title":"Verifying our Pod has the expected environment variables"},{"location":"deploying-declaratively/","text":"Deploying declaratively with the Kubernetes dashboard \u00b6 The Kubernetes dashboard helps to visualize and understand what is happening with your deployments and the cluster itself. We're going to install it as an example deploying declaratively in Kubernetes. Warning The instructions below are suitable for a local development environment only ! This article from Heptio is essential reading for securing the Kubernetes dashboard . Step 1. Install it \u00b6 Let's use the kubectl (Kube Control) CLI tool to install a the required objects for the dashboard. 1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml By the end of the training, you will understand this entire file. Step 2. Kubernetes dashboard access \u00b6 Even for our development environment, we don't want to expose the dashboard publicly, so we have two simple options: 1 kubectl proxy or 1 kubectl port-forward I prefer the kubectl port-forward option as when using Docker for Desktop, you access it at localhost: . kubectl proxy may be better if you need to access other internal services, or if the self-signed certification is giving you problems. It's up to you and below, we'll use both. Step 2.1 Kubernetes Dashboard with Port Forwarding \u00b6 The kubectl port-forward command needs the Pod name, the host machine port, and the local port. !!! note: The following code presumes the existence of bash so if you're on Windows, you'll have to grab the pod name and insert it into the port forward command manually. Grab the name of the Pod: 1 kube_dashboard_name=$(kubectl get pods --namespace=kube-system | grep kubernetes-dashboard | awk '{print $1}') Setup the port forwarding 1 kubectl port-forward ${ kube_dashboard_name } 30000:8443 --namespace=kube-system Now you can access the dashboard at https://localhost:30000 Want to run the port forwarding in the background with a single command? From the learn-kubernetes directory. 1 make k8s-dashboard-up To kill the port forwarding process and remove the dashboard. 1 make k8s-dashboard-down Step 2.2 Kubernetes Dashboard with kubectl proxy \u00b6 The command is easy to remember, the URL not so much: 1 kubectl proxy Then go to http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview Note The kubectl proxy makes it possible to access any Service. TODO: LINK. Step 3. Access the Dashboard \u00b6 If this is the first time you've accessed the dashboard, you'll be greeted with a setup screen. We're going to click SKIP as we're using the Dashboard for our development environment. Kubernetes Dashboard in Production \u00b6 Access should be controlled via a Service token and it shoule be used as a read-only dashboard, not as a way to manage your cluster. Danger Never expose it publicly! Read more about securing the Kubernetes dashboard .","title":"Deploying Declaratively"},{"location":"deploying-declaratively/#deploying-declaratively-with-the-kubernetes-dashboard","text":"The Kubernetes dashboard helps to visualize and understand what is happening with your deployments and the cluster itself. We're going to install it as an example deploying declaratively in Kubernetes. Warning The instructions below are suitable for a local development environment only ! This article from Heptio is essential reading for securing the Kubernetes dashboard .","title":"Deploying declaratively with the Kubernetes dashboard"},{"location":"deploying-declaratively/#step-1-install-it","text":"Let's use the kubectl (Kube Control) CLI tool to install a the required objects for the dashboard. 1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml By the end of the training, you will understand this entire file.","title":"Step 1. Install it"},{"location":"deploying-declaratively/#step-2-kubernetes-dashboard-access","text":"Even for our development environment, we don't want to expose the dashboard publicly, so we have two simple options: 1 kubectl proxy or 1 kubectl port-forward I prefer the kubectl port-forward option as when using Docker for Desktop, you access it at localhost: . kubectl proxy may be better if you need to access other internal services, or if the self-signed certification is giving you problems. It's up to you and below, we'll use both.","title":"Step 2. Kubernetes dashboard access"},{"location":"deploying-declaratively/#step-21-kubernetes-dashboard-with-port-forwarding","text":"The kubectl port-forward command needs the Pod name, the host machine port, and the local port. !!! note: The following code presumes the existence of bash so if you're on Windows, you'll have to grab the pod name and insert it into the port forward command manually. Grab the name of the Pod: 1 kube_dashboard_name=$(kubectl get pods --namespace=kube-system | grep kubernetes-dashboard | awk '{print $1}') Setup the port forwarding 1 kubectl port-forward ${ kube_dashboard_name } 30000:8443 --namespace=kube-system Now you can access the dashboard at https://localhost:30000 Want to run the port forwarding in the background with a single command? From the learn-kubernetes directory. 1 make k8s-dashboard-up To kill the port forwarding process and remove the dashboard. 1 make k8s-dashboard-down","title":"Step 2.1 Kubernetes Dashboard with Port Forwarding"},{"location":"deploying-declaratively/#step-22-kubernetes-dashboard-with-kubectl-proxy","text":"The command is easy to remember, the URL not so much: 1 kubectl proxy Then go to http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview Note The kubectl proxy makes it possible to access any Service. TODO: LINK.","title":"Step 2.2 Kubernetes Dashboard with kubectl proxy"},{"location":"deploying-declaratively/#step-3-access-the-dashboard","text":"If this is the first time you've accessed the dashboard, you'll be greeted with a setup screen. We're going to click SKIP as we're using the Dashboard for our development environment.","title":"Step 3. Access the Dashboard"},{"location":"deploying-declaratively/#kubernetes-dashboard-in-production","text":"Access should be controlled via a Service token and it shoule be used as a read-only dashboard, not as a way to manage your cluster. Danger Never expose it publicly! Read more about securing the Kubernetes dashboard .","title":"Kubernetes Dashboard in Production"},{"location":"deployments/","text":"Deployments \u00b6 Before we create the deployment, let's take a look at manifests/deployment.yaml file to see what resources it depends on. Make sure you have created the required ConfigMaps and Secrets. Then create the Deployment. 1 kubectl apply -f manifests/deployment.yaml Get the state of the Deployment: 1 2 kubectl get deployments kubectl describe deployment kuard Observing the change to our Pods \u00b6 In a new terminal window, observe the state change to the list of Pods. We're going to keep this around for this entire lab. 1 make watch-pods Scale our Pods using Deployments \u00b6 Update spec.replicas in deployment.yaml to be 3 , then deploy the change: 1 kubectl apply -f manifests/deployment.yaml Let's scale up the imperative way: 1 kubectl scale --replicas=5 deployment kuard Notice the old pods don't get deleted straight away. This is because of spec.minReadySeconds value of 30 seconds. Update spec.replicas in deployment.yaml to be 20 , then deploy our changes. Did all of the Pods come up? Let's go checkout the Kubernetes dashboard. Update spec.replicas in deployment.yaml to be 2 again and apply the change. <!-- Rolling Back to a Previous Revision \u00b6 Let's say that something went wrong when we upgraded to the newer kuard container and we want to roll back: 1 kubectl rollout undo deployment kuard --&gt; Update the deployment to Deploy a new version of the kuard container \u00b6 Update your deployment.yaml file, changing the image tag to 2 and add the following under spec.template.metadata in order to provide a reason for the manifest change. This is something that would be template driven by your CD system. 1 2 annotations : kubernetes . io / change - cause : 'Upgraded to version 2.' Then apply your change: 1 kubectl apply -f manifests/deployment.yaml Observe the deployment history: 1 kubectl rollout history deployment kuard Debugging Tip: Isolating a Pod from a ReplicaSet \u00b6 If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong. To do this, you can change or overwrite the labels (depending upon the RepicaSet label selector). Let's take one of the deployment pods out of service. 1 2 3 4 5 6 7 8 # Get a pod name kubectl get pods # Take it out of service by changing its label # NOTE: Changing the label value is only to disassociate it the RepicaSet and Service. It has nothing to do with the actual health of the Pod and Kubernetes does not care about the value. kubectl label pods kuard-- $ID --overwrite app=kuard-quarantined. This will cause Kubernetes to disassociate that Pod with the deployment which in turn, will cause Kubernetes to create a new Pod. As the renamed Pod is now un-managed, you can exec into it for investigation. Be aware that because it is now un-managed, it will need to be deleted manually.","title":"Deployments"},{"location":"deployments/#deployments","text":"Before we create the deployment, let's take a look at manifests/deployment.yaml file to see what resources it depends on. Make sure you have created the required ConfigMaps and Secrets. Then create the Deployment. 1 kubectl apply -f manifests/deployment.yaml Get the state of the Deployment: 1 2 kubectl get deployments kubectl describe deployment kuard","title":"Deployments"},{"location":"deployments/#observing-the-change-to-our-pods","text":"In a new terminal window, observe the state change to the list of Pods. We're going to keep this around for this entire lab. 1 make watch-pods","title":"Observing the change to our Pods"},{"location":"deployments/#scale-our-pods-using-deployments","text":"Update spec.replicas in deployment.yaml to be 3 , then deploy the change: 1 kubectl apply -f manifests/deployment.yaml Let's scale up the imperative way: 1 kubectl scale --replicas=5 deployment kuard Notice the old pods don't get deleted straight away. This is because of spec.minReadySeconds value of 30 seconds. Update spec.replicas in deployment.yaml to be 20 , then deploy our changes. Did all of the Pods come up? Let's go checkout the Kubernetes dashboard. Update spec.replicas in deployment.yaml to be 2 again and apply the change. <!--","title":"Scale our Pods using Deployments"},{"location":"deployments/#rolling-back-to-a-previous-revision","text":"Let's say that something went wrong when we upgraded to the newer kuard container and we want to roll back: 1 kubectl rollout undo deployment kuard --&gt;","title":"Rolling Back to a Previous Revision"},{"location":"deployments/#update-the-deployment-to-deploy-a-new-version-of-the-kuard-container","text":"Update your deployment.yaml file, changing the image tag to 2 and add the following under spec.template.metadata in order to provide a reason for the manifest change. This is something that would be template driven by your CD system. 1 2 annotations : kubernetes . io / change - cause : 'Upgraded to version 2.' Then apply your change: 1 kubectl apply -f manifests/deployment.yaml Observe the deployment history: 1 kubectl rollout history deployment kuard","title":"Update the deployment to Deploy a new version of the kuard container"},{"location":"deployments/#debugging-tip-isolating-a-pod-from-a-replicaset","text":"If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong. To do this, you can change or overwrite the labels (depending upon the RepicaSet label selector). Let's take one of the deployment pods out of service. 1 2 3 4 5 6 7 8 # Get a pod name kubectl get pods # Take it out of service by changing its label # NOTE: Changing the label value is only to disassociate it the RepicaSet and Service. It has nothing to do with the actual health of the Pod and Kubernetes does not care about the value. kubectl label pods kuard-- $ID --overwrite app=kuard-quarantined. This will cause Kubernetes to disassociate that Pod with the deployment which in turn, will cause Kubernetes to create a new Pod. As the renamed Pod is now un-managed, you can exec into it for investigation. Be aware that because it is now un-managed, it will need to be deleted manually.","title":"Debugging Tip: Isolating a Pod from a ReplicaSet"},{"location":"kubectl/","text":"Kubectl essentials \u00b6 The Kubernetes documentation has its own kubectl cheat sheet . It's essential reading and you should definitely look at it but for now, here are some important and some lesser known commands. Many basic ones are covered in the other labs too. kubectl \u00b6 Perhaps the most important command that you may not have thought to run is: 1 kubectl This gives you an overview of the various commands.The grouping by purpose is very useful for knowing if a particular command would be useful for what you need to accomplish. kubectl global options \u00b6 kubectl has a list of options that can be passed to any command (e.g. --namespace ) 1 kubectl options kubectl bash completion \u00b6 This is essential. Not as easy as using the bash-completion system for homebrew but still easy. 1 2 3 4 source &lt;(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first. # add autocomplete permanently to your bash shell. echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bash_profile View pods \u00b6 Sometimes you need more control over the pods that are returned by kubectl . 1 2 3 4 kubectl get pods --include-uninitialized # Status could be one of Pending, Running, Succeeded, Failed, or Unknown kubectl get pods --field-selector=status.phase=Running Diff configurations between local and live versions \u00b6 While you can use something like kubectl get pod kuard-pod -o yaml , that will return the entire YAML stores in etcd. What about if you wanted to compare what Kubernetes has vs. what you have locally. 1 2 kubectl apply -f manifests/pod.yaml kubectl alpha diff -f manifests/pod.yaml This could be used to measure drift between resources in source control vs. resources in the cluster. It's also an awesome way to learn what other fields your specifications could have. For example, I could add successThreshold to my livenessProbe properties. kubectl explain \u00b6 Documentation for command line tools isn't always easy to understand. With Kubernetes, it's awesome! 1 kubectl explain deployment Oops. Notice VERSION: extensions/v1beta1 at the top?. We're looking at documentation for the first version of Deployments which is now deprecated. What we need is the deployments from the apps API group. 1 kubectl explain deployment --api-version=apps/v1 Now lets get more specific about what we want to learn about Deployments. 1 2 kubectl explain deployment.spec --api-version=apps/v1 kubectl explain deployment.spec.strategy --api-version=apps/v1 Scale a deployment \u00b6 This is important if you need to scale a deployment immediately but the advisable method is change the manifest YAML and change the replias that way. 1 2 # Note: This won't work as we don't have a Deployment named `my-app`. It's just an example. kubectl scale --replicas=3 deployment/my-app A debug pod \u00b6 To deug the state of an application from within the Namespace, I'd recommend deploying a short-lived debug Pod instead of kubectl exec to access and existing Pod. You could even create a specialized debug image that has all the typical tools you need when debugging. 1 kubectl run debug-shell --rm -it --image ubuntu:latest -- bash Cluster event stream \u00b6 While you're probably better served by specially designed service, this is a good substitute in the absence of that. 1 kubectl get events --sort-by=.metadata.creationTimestamp -o custom-columns=CREATED:.metadata.creationTimestamp,NAMESPACE:involvedObject.namespace,NAME:.involvedObject.name,REASON:.reason,KIND:.involvedObject.kind,MESSAGE:.message -w --all-namespaces Tail Pod logs \u00b6 Kail is an awesome tool for tailing the logs of multiple Pods using a varied set of selectors. What's great, is that we can deploy it into our cluster so there is nothing to install locally. To tail all pods in the cluster (probably only useful on your local instance). TODO: Test 1 2 kubectl apply -f manifests/pod.yaml kubectl run -it --rm -l kail.ignore=true --restart=Never --image=abozanich/kail kail -- --pod kuard-pod Tail the kuard-pod . 1 kubectl run -it --rm -l kail.ignore=true --restart=Never --image=abozanich/kail kail -- --pod kuard-pod Force delete a Pod \u00b6 Sometimes, a Pod can get into a state where you cannot kill it. This command should be a last resort. 1 kubectl delete pod $POD_NAME --force --grace-period=0 Caution! Delete all the objects from a namespace \u00b6 This is handy during development when you want to clear out all objects in a namespace. Always specify the --namespace for safety. 1 kubectl delete pods,services,configmaps,secrets,replicasets,deployments,jobs,cronjobs,daemonsets,statefulsets,podsecuritypolicies --all --namespace=learn-k8s You could achieve the same thing by deleting the Namespace. Just remember to create it again.","title":"Kubectl Essentials"},{"location":"kubectl/#kubectl-essentials","text":"The Kubernetes documentation has its own kubectl cheat sheet . It's essential reading and you should definitely look at it but for now, here are some important and some lesser known commands. Many basic ones are covered in the other labs too.","title":"Kubectl essentials"},{"location":"kubectl/#kubectl","text":"Perhaps the most important command that you may not have thought to run is: 1 kubectl This gives you an overview of the various commands.The grouping by purpose is very useful for knowing if a particular command would be useful for what you need to accomplish.","title":"kubectl"},{"location":"kubectl/#kubectl-global-options","text":"kubectl has a list of options that can be passed to any command (e.g. --namespace ) 1 kubectl options","title":"kubectl global options"},{"location":"kubectl/#kubectl-bash-completion","text":"This is essential. Not as easy as using the bash-completion system for homebrew but still easy. 1 2 3 4 source &lt;(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first. # add autocomplete permanently to your bash shell. echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bash_profile","title":"kubectl bash completion"},{"location":"kubectl/#view-pods","text":"Sometimes you need more control over the pods that are returned by kubectl . 1 2 3 4 kubectl get pods --include-uninitialized # Status could be one of Pending, Running, Succeeded, Failed, or Unknown kubectl get pods --field-selector=status.phase=Running","title":"View pods"},{"location":"kubectl/#diff-configurations-between-local-and-live-versions","text":"While you can use something like kubectl get pod kuard-pod -o yaml , that will return the entire YAML stores in etcd. What about if you wanted to compare what Kubernetes has vs. what you have locally. 1 2 kubectl apply -f manifests/pod.yaml kubectl alpha diff -f manifests/pod.yaml This could be used to measure drift between resources in source control vs. resources in the cluster. It's also an awesome way to learn what other fields your specifications could have. For example, I could add successThreshold to my livenessProbe properties.","title":"Diff configurations between local and live versions"},{"location":"kubectl/#kubectl-explain","text":"Documentation for command line tools isn't always easy to understand. With Kubernetes, it's awesome! 1 kubectl explain deployment Oops. Notice VERSION: extensions/v1beta1 at the top?. We're looking at documentation for the first version of Deployments which is now deprecated. What we need is the deployments from the apps API group. 1 kubectl explain deployment --api-version=apps/v1 Now lets get more specific about what we want to learn about Deployments. 1 2 kubectl explain deployment.spec --api-version=apps/v1 kubectl explain deployment.spec.strategy --api-version=apps/v1","title":"kubectl explain"},{"location":"kubectl/#scale-a-deployment","text":"This is important if you need to scale a deployment immediately but the advisable method is change the manifest YAML and change the replias that way. 1 2 # Note: This won't work as we don't have a Deployment named `my-app`. It's just an example. kubectl scale --replicas=3 deployment/my-app","title":"Scale a deployment"},{"location":"kubectl/#a-debug-pod","text":"To deug the state of an application from within the Namespace, I'd recommend deploying a short-lived debug Pod instead of kubectl exec to access and existing Pod. You could even create a specialized debug image that has all the typical tools you need when debugging. 1 kubectl run debug-shell --rm -it --image ubuntu:latest -- bash","title":"A debug pod"},{"location":"kubectl/#cluster-event-stream","text":"While you're probably better served by specially designed service, this is a good substitute in the absence of that. 1 kubectl get events --sort-by=.metadata.creationTimestamp -o custom-columns=CREATED:.metadata.creationTimestamp,NAMESPACE:involvedObject.namespace,NAME:.involvedObject.name,REASON:.reason,KIND:.involvedObject.kind,MESSAGE:.message -w --all-namespaces","title":"Cluster event stream"},{"location":"kubectl/#tail-pod-logs","text":"Kail is an awesome tool for tailing the logs of multiple Pods using a varied set of selectors. What's great, is that we can deploy it into our cluster so there is nothing to install locally. To tail all pods in the cluster (probably only useful on your local instance). TODO: Test 1 2 kubectl apply -f manifests/pod.yaml kubectl run -it --rm -l kail.ignore=true --restart=Never --image=abozanich/kail kail -- --pod kuard-pod Tail the kuard-pod . 1 kubectl run -it --rm -l kail.ignore=true --restart=Never --image=abozanich/kail kail -- --pod kuard-pod","title":"Tail Pod logs"},{"location":"kubectl/#force-delete-a-pod","text":"Sometimes, a Pod can get into a state where you cannot kill it. This command should be a last resort. 1 kubectl delete pod $POD_NAME --force --grace-period=0","title":"Force delete a Pod"},{"location":"kubectl/#caution-delete-all-the-objects-from-a-namespace","text":"This is handy during development when you want to clear out all objects in a namespace. Always specify the --namespace for safety. 1 kubectl delete pods,services,configmaps,secrets,replicasets,deployments,jobs,cronjobs,daemonsets,statefulsets,podsecuritypolicies --all --namespace=learn-k8s You could achieve the same thing by deleting the Namespace. Just remember to create it again.","title":"Caution! Delete all the objects from a namespace"},{"location":"namespaces/","text":"Namespaces \u00b6 Namespaces allow you to break a Kubernetes cluster into several virtual clusters. What namespaces exist for your cluster? 1 kubectl get namespaces What objects are in a particular namespace? 1 kubectl get all --namespace=kube-system Let's create the learn-k8s namespace that we'll use at times during this training. 1 kubectl apply -f manifests/namespace.yaml Verify it succeeded. 1 kubectl get namespaces Let's create the kuard-pod in the default namespace for verifying what namespace we're pointing to later. 1 kubectl apply -f manifests/pod.yaml The kubectl context \u00b6 A context is just that, a context for what cluster, which user and which namespace should be used for deploying resources. You can use contexts for switching between different clusters or even the same cluster but with a different user, different namespace or both. We're going to create a context for the docker-for-desktop cluster that sets the namespace to learn-k8s so all of the resources you deploy for these labs don't interfere with any resources you may have in the default namespace. That's what namespaces are for! To keep things separated. We'll call the context learn-k8s (first positional argument). If you are on Docker for Desktop : 1 kubectl config set-context learn-k8s --cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s If you are on Minikube : 1 kubectl config set-context learn-k8s --cluster=minikube --user=minikube --namespace=learn-k8s But this has only created the context. Let's tell kubectl to use our new learn-k8s context. 1 kubectl config use-context learn-k8s As a test, let's create kuard-pod and inspect the Pod properties to see which namespace it was created in. 1 2 kubectl apply -f manifests/pod.yaml echo $(kubectl get pod kuard-pod -o \"jsonpath={.metadata['namespace']}\") Cleanup the kuard-pod : 1 kubectl delete -f manifests/pod.yaml Note You can also switch the context by using the Docker toolbar app.","title":"Namespaces"},{"location":"namespaces/#namespaces","text":"Namespaces allow you to break a Kubernetes cluster into several virtual clusters. What namespaces exist for your cluster? 1 kubectl get namespaces What objects are in a particular namespace? 1 kubectl get all --namespace=kube-system Let's create the learn-k8s namespace that we'll use at times during this training. 1 kubectl apply -f manifests/namespace.yaml Verify it succeeded. 1 kubectl get namespaces Let's create the kuard-pod in the default namespace for verifying what namespace we're pointing to later. 1 kubectl apply -f manifests/pod.yaml","title":"Namespaces"},{"location":"namespaces/#the-kubectl-context","text":"A context is just that, a context for what cluster, which user and which namespace should be used for deploying resources. You can use contexts for switching between different clusters or even the same cluster but with a different user, different namespace or both. We're going to create a context for the docker-for-desktop cluster that sets the namespace to learn-k8s so all of the resources you deploy for these labs don't interfere with any resources you may have in the default namespace. That's what namespaces are for! To keep things separated. We'll call the context learn-k8s (first positional argument). If you are on Docker for Desktop : 1 kubectl config set-context learn-k8s --cluster=docker-for-desktop-cluster --user=docker-for-desktop --namespace=learn-k8s If you are on Minikube : 1 kubectl config set-context learn-k8s --cluster=minikube --user=minikube --namespace=learn-k8s But this has only created the context. Let's tell kubectl to use our new learn-k8s context. 1 kubectl config use-context learn-k8s As a test, let's create kuard-pod and inspect the Pod properties to see which namespace it was created in. 1 2 kubectl apply -f manifests/pod.yaml echo $(kubectl get pod kuard-pod -o \"jsonpath={.metadata['namespace']}\") Cleanup the kuard-pod : 1 kubectl delete -f manifests/pod.yaml Note You can also switch the context by using the Docker toolbar app.","title":"The kubectl context"},{"location":"pods/","text":"Pods Lab \u00b6 In this lab, we will deploy a Pod, the unit of compute to our Kubernetes cluster. A real application would never have just a single Pod, we're keeping things simple for now and we'll get to multiple Pods later when we cover Deployments. Note You will notice that we always use the declarative form for changing the state of our Kubernetes cluster throughout these labs, and I encourage you to do the same. See the page on Object Management using kubetcl for more info. Introducing the Kubernetes Up and Running Demo (kuard) Pod \u00b6 From the excellent Kubernetes Up and Running book, we're going to use the kuard container throughout these labs as its a fantastic container to use for debugging and learning about Kubernetes. Warning THe kuard container is designed for learning and should never be run on a production cluster. Use it on a local cluster only. Create the kuard Pod the declarative way (the right way): 1 kubectl apply -f manifests/pod.yaml Verify that the container is running in the Pod and is listening on port 8080: 1 2 3 4 5 # Eample of using a Go template kubectl get pod kuard-pod --template=' {{ ( index ( index .spec.containers 0 ) .ports 0 ) .containerPort }}{{ \"\\n\" }} ' # I prefer using json path as I think it's easier to use and read kubectl get pod kuard-pod -o \"jsonpath={.spec.containers[0].ports[0].containerPort}\" Pod names \u00b6 Pod names must be unique for a Namespace. Let's try create a Pod named kuard-pod in our learn-k8s namespace. 1 2 # Using create here as apply will think we're applying updates to the existing kuard-pod kubectl create -f manifests/pod.yaml You should have gotten a AlreadyExists error. Port forwarding \u00b6 We can't reach the kuard-pod because everything in the cluster is private unless exposed by a Service. To get the kuard-pod , we'll use port forwarding: 1 kubectl port-forward kuard-pod 8080:8080 View the API in your browser at http://localhost:8080/ Kill the port-forwarding process in your terminal by sending a sigal interupt (SIGINT) by pressing Ctrl+C. Let's inspect our Pod in a few different ways: 1 2 3 4 5 kubectl get pod kuard-pod kubectl get pod kuard-pod -o wide kubectl describe pod kuard-pod kubectl get pod kuard-pod -o yaml kubectl get pod kuard-pod -o json | jq These commands are not specific to Pods. This is how we data about all of the resources avaialble to our cluster. Container logs \u00b6 Let's view the logs for our Pod: 1 kubectl logs kuard-pod When you have multiple containers in your Pod, you'll need to specify which container you want the logs for. 1 2 kubectl logs kuard-pod $CONTAINER_NAME kubectl logs kuard-pod kuard If we want to tail the logs, we need to supply the -f flag. 1 kubectl logs kuard-pod kuard -f We can see regular requests for /healthy and /ready . This is being called by the kubelet that is responsible for monitoring the health of the pods on each worker node. Accessing a Pod \u00b6 Wouldn't it be great if we could get into a container inside our Pod, as easily as the docker container exec command. 1 kubectl exec -it kuard-pod sh By default, kubectl will execute the command against the first container in the spec.containers list. use the --container (or -c ) and the container name to choose which container to exec to: 1 kubectl exec -it kuard-pod --container kuard sh Now that we're inside the container, we can run commands: 1 printenv We can run any command against the container though (we don't have to have a shell session first). 1 kubectl exec -it kuard-pod --container kuard printenv Liveness probe \u00b6 Now let's see how Kubernetes reacts to the kuard container being in an unhealthy state. 1 kubectl port-forward kuard-pod 8080:8080 Then in another terminal: 1 make event-stream Open the liveness page for the kuard container and click on the Fail link. Observe that Kubernetes imediately killed the Pod as soon as the container's .spec.containers.livenessProbe.failureThreshold was exceeded. It was then replaced with a brand new Pod. When a Pod dies, any file system changes die with it, and that's a good thing. Stateless applications are predictable as we always know they start with the same file system contents. Kill the make event-stream process in your terminal by sending a sigal interupt (SIGINT) by pressing Ctrl+C. Readiness probe \u00b6 When it comes to readiness probes, Kubernetes is more tolerant. A successful readiness probe result indicates the container is ready to do whatever it does. That's different to a liveness probe which indicates whether the container is dead or alive. In the terminal that you previously ran make event-stream , run: 1 watch kubectl get pod kuard-pod -o wide Open the [readiness page]http://localhost:8080/-/readiness) for the kuard container and click on the Fail link. Once the readiness probe exceeds the failureThreshold value, the READY values changes to 0/1 . But notice that the Pod did not get killed. That's because it is still alive. If on the [readiness page]http://localhost:8080/-/readiness), we click on the Succeed link, we will see READY change to 1/1 . When your Pod exceeds its allotted memory \u00b6 Kubernetes has no tolerance for containers that exceed their memory consumption limits. Let's open the Memory page and click on the Allocate 500 MiB link. We see Kubernetes kill the Pod soon after, noticing the STATUS briefly changing to OOMKilled , then to CrashLoopBackOff , then to RUNNING . The transition to the CrashLoopBackOff STATUS is important. In order to protect the Nodes, every time your Pod is killed due to a problem, the time between entering CrashLoopBackOff and RUNNING increases. A debugging Pod \u00b6 Sometimes you may want to inject a Pod into the current namespace for introspection purposes. First, let's get the IP of the kuard Pod. We'll use this in our debugging container. 1 echo $(kubectl get pod kuard-pod -o \"jsonpath={.status.podIP}\") Then, with a single command, we can launch a deployment and exec into our debugging container. Note I'm now aware this command is now deprecated and am investigating an alternative. make debug-container Now let's try hitting the Pod directly using its IP address. 1 wget $IP:8080 -q -O - We're going to use this debug container again when we look at Services. Exit from the debug container. As a result of exiting, it should also kill the deployment. Sometimes, it does not. Check by running: 1 kubectl get deployments If you can still see your debug container running, then use the following command to delete it. 1 make kill-debug-container Finally, remove the kuard Pod so we're in a clean state for the next lab. 1 kubectl delete -f manifests/pod.yaml","title":"Pods"},{"location":"pods/#pods-lab","text":"In this lab, we will deploy a Pod, the unit of compute to our Kubernetes cluster. A real application would never have just a single Pod, we're keeping things simple for now and we'll get to multiple Pods later when we cover Deployments. Note You will notice that we always use the declarative form for changing the state of our Kubernetes cluster throughout these labs, and I encourage you to do the same. See the page on Object Management using kubetcl for more info.","title":"Pods Lab"},{"location":"pods/#introducing-the-kubernetes-up-and-running-demo-kuard-pod","text":"From the excellent Kubernetes Up and Running book, we're going to use the kuard container throughout these labs as its a fantastic container to use for debugging and learning about Kubernetes. Warning THe kuard container is designed for learning and should never be run on a production cluster. Use it on a local cluster only. Create the kuard Pod the declarative way (the right way): 1 kubectl apply -f manifests/pod.yaml Verify that the container is running in the Pod and is listening on port 8080: 1 2 3 4 5 # Eample of using a Go template kubectl get pod kuard-pod --template=' {{ ( index ( index .spec.containers 0 ) .ports 0 ) .containerPort }}{{ \"\\n\" }} ' # I prefer using json path as I think it's easier to use and read kubectl get pod kuard-pod -o \"jsonpath={.spec.containers[0].ports[0].containerPort}\"","title":"Introducing the Kubernetes Up and Running Demo (kuard) Pod"},{"location":"pods/#pod-names","text":"Pod names must be unique for a Namespace. Let's try create a Pod named kuard-pod in our learn-k8s namespace. 1 2 # Using create here as apply will think we're applying updates to the existing kuard-pod kubectl create -f manifests/pod.yaml You should have gotten a AlreadyExists error.","title":"Pod names"},{"location":"pods/#port-forwarding","text":"We can't reach the kuard-pod because everything in the cluster is private unless exposed by a Service. To get the kuard-pod , we'll use port forwarding: 1 kubectl port-forward kuard-pod 8080:8080 View the API in your browser at http://localhost:8080/ Kill the port-forwarding process in your terminal by sending a sigal interupt (SIGINT) by pressing Ctrl+C. Let's inspect our Pod in a few different ways: 1 2 3 4 5 kubectl get pod kuard-pod kubectl get pod kuard-pod -o wide kubectl describe pod kuard-pod kubectl get pod kuard-pod -o yaml kubectl get pod kuard-pod -o json | jq These commands are not specific to Pods. This is how we data about all of the resources avaialble to our cluster.","title":"Port forwarding"},{"location":"pods/#container-logs","text":"Let's view the logs for our Pod: 1 kubectl logs kuard-pod When you have multiple containers in your Pod, you'll need to specify which container you want the logs for. 1 2 kubectl logs kuard-pod $CONTAINER_NAME kubectl logs kuard-pod kuard If we want to tail the logs, we need to supply the -f flag. 1 kubectl logs kuard-pod kuard -f We can see regular requests for /healthy and /ready . This is being called by the kubelet that is responsible for monitoring the health of the pods on each worker node.","title":"Container logs"},{"location":"pods/#accessing-a-pod","text":"Wouldn't it be great if we could get into a container inside our Pod, as easily as the docker container exec command. 1 kubectl exec -it kuard-pod sh By default, kubectl will execute the command against the first container in the spec.containers list. use the --container (or -c ) and the container name to choose which container to exec to: 1 kubectl exec -it kuard-pod --container kuard sh Now that we're inside the container, we can run commands: 1 printenv We can run any command against the container though (we don't have to have a shell session first). 1 kubectl exec -it kuard-pod --container kuard printenv","title":"Accessing a Pod"},{"location":"pods/#liveness-probe","text":"Now let's see how Kubernetes reacts to the kuard container being in an unhealthy state. 1 kubectl port-forward kuard-pod 8080:8080 Then in another terminal: 1 make event-stream Open the liveness page for the kuard container and click on the Fail link. Observe that Kubernetes imediately killed the Pod as soon as the container's .spec.containers.livenessProbe.failureThreshold was exceeded. It was then replaced with a brand new Pod. When a Pod dies, any file system changes die with it, and that's a good thing. Stateless applications are predictable as we always know they start with the same file system contents. Kill the make event-stream process in your terminal by sending a sigal interupt (SIGINT) by pressing Ctrl+C.","title":"Liveness probe"},{"location":"pods/#readiness-probe","text":"When it comes to readiness probes, Kubernetes is more tolerant. A successful readiness probe result indicates the container is ready to do whatever it does. That's different to a liveness probe which indicates whether the container is dead or alive. In the terminal that you previously ran make event-stream , run: 1 watch kubectl get pod kuard-pod -o wide Open the [readiness page]http://localhost:8080/-/readiness) for the kuard container and click on the Fail link. Once the readiness probe exceeds the failureThreshold value, the READY values changes to 0/1 . But notice that the Pod did not get killed. That's because it is still alive. If on the [readiness page]http://localhost:8080/-/readiness), we click on the Succeed link, we will see READY change to 1/1 .","title":"Readiness probe"},{"location":"pods/#when-your-pod-exceeds-its-allotted-memory","text":"Kubernetes has no tolerance for containers that exceed their memory consumption limits. Let's open the Memory page and click on the Allocate 500 MiB link. We see Kubernetes kill the Pod soon after, noticing the STATUS briefly changing to OOMKilled , then to CrashLoopBackOff , then to RUNNING . The transition to the CrashLoopBackOff STATUS is important. In order to protect the Nodes, every time your Pod is killed due to a problem, the time between entering CrashLoopBackOff and RUNNING increases.","title":"When your Pod exceeds its allotted memory"},{"location":"pods/#a-debugging-pod","text":"Sometimes you may want to inject a Pod into the current namespace for introspection purposes. First, let's get the IP of the kuard Pod. We'll use this in our debugging container. 1 echo $(kubectl get pod kuard-pod -o \"jsonpath={.status.podIP}\") Then, with a single command, we can launch a deployment and exec into our debugging container. Note I'm now aware this command is now deprecated and am investigating an alternative. make debug-container Now let's try hitting the Pod directly using its IP address. 1 wget $IP:8080 -q -O - We're going to use this debug container again when we look at Services. Exit from the debug container. As a result of exiting, it should also kill the deployment. Sometimes, it does not. Check by running: 1 kubectl get deployments If you can still see your debug container running, then use the following command to delete it. 1 make kill-debug-container Finally, remove the kuard Pod so we're in a clean state for the next lab. 1 kubectl delete -f manifests/pod.yaml","title":"A debugging Pod"},{"location":"rbac/","text":"RBAC \u00b6 Role-Based Access Control (RBAC) controls access to the Kubernetes API and restricts what objects in what namespaces an authenticated user is authorizd to access. RBAC has been GA in Kubernetes since 1.8 (late 2017) so most Kubernetes clusters have it enabled by default. Kubernetes Cluster Roles \u00b6 Kubernetes comes with a set of default cluster roles: 1 kubectl get clusterroles Let's take a look at the view ClusterRole. 1 kubectl get clusterrole view -o yaml Kubernetes Roles and Role Bindings \u00b6 Running this will likely not return results because we haven't created any Roles in our learn-k8s Namespace: 1 kubectl get roles Let's look in the kube-system namespace and inspect the Role and RoleBinding for the Kubernetes dashboard: 1 2 3 4 kubectl get roles --namespace=kube-system kubectl get role kubernetes-dashboard-minimal -o yaml --namespace=kube-system kubectl get rolebindings --namespace=kube-system kubectl get rolebinding kubernetes-dashboard-minimal -o yaml --namespace=kube-system Restricting a user to a namespace \u00b6 Let's add a new user that can manage the resources for that namespace. 1 kubectl apply -f manifests/rbac-role-namespace-viewer.yaml Taking a look at manifests/rbac-role-namespace-viewer.yaml , we've created three resources: ServiceAccount Role RoleBinding The Role has full access to all resources (including batch objects such as jobs). Because it is a Role, it can only be applied to a Namespace. Retrieve certificate credentials \u00b6 In creating the ServiceAccount, Kubernetes created a secret that contains the CA and token we need for authentication. 1 kubectl describe serviceaccount learn-k8s-user The name of the secret is in the Tokens value. 1 SECRET_NAME=$(kubectl get serviceaccount learn-k8s-user -o \"jsonpath={.secrets[0]['name']}\") Now let's get the token: 1 kubectl get secret $SECRET_NAME -o \"jsonpath={.data.token}\" And CA data: 1 kubectl get secret $SECRET_NAME -o \"jsonpath={.data['ca\\.crt']}\" Now, let's append a user to our Kube config file ( ~/.kube/config ), adding another entry to users : 1 2 3 4 - name: learn-k8s user: client-key-data: $CA token: $TOKEN Now we can test out the access capabilities of our learn-k8s user: 1 kubectl get secrets --as=learn-k8s --namespace=kube-system You should have received an error indicating you are not allowed to list secrets from the kube-system namespace. The --as=learn-k8s is a great option for testing user access. Now let's change our learn-k8s context entry in the Kube config file to use our new learn-k8s-user user. Now if you run: 1 kubectl get secrets --namespace=kube-system You'll get the same error. Note If you're ever running commands that need Cluster level access, switch back to original context setup for your cluster.)","title":"RBAC"},{"location":"rbac/#rbac","text":"Role-Based Access Control (RBAC) controls access to the Kubernetes API and restricts what objects in what namespaces an authenticated user is authorizd to access. RBAC has been GA in Kubernetes since 1.8 (late 2017) so most Kubernetes clusters have it enabled by default.","title":"RBAC"},{"location":"rbac/#kubernetes-cluster-roles","text":"Kubernetes comes with a set of default cluster roles: 1 kubectl get clusterroles Let's take a look at the view ClusterRole. 1 kubectl get clusterrole view -o yaml","title":"Kubernetes Cluster Roles"},{"location":"rbac/#kubernetes-roles-and-role-bindings","text":"Running this will likely not return results because we haven't created any Roles in our learn-k8s Namespace: 1 kubectl get roles Let's look in the kube-system namespace and inspect the Role and RoleBinding for the Kubernetes dashboard: 1 2 3 4 kubectl get roles --namespace=kube-system kubectl get role kubernetes-dashboard-minimal -o yaml --namespace=kube-system kubectl get rolebindings --namespace=kube-system kubectl get rolebinding kubernetes-dashboard-minimal -o yaml --namespace=kube-system","title":"Kubernetes Roles and Role Bindings"},{"location":"rbac/#restricting-a-user-to-a-namespace","text":"Let's add a new user that can manage the resources for that namespace. 1 kubectl apply -f manifests/rbac-role-namespace-viewer.yaml Taking a look at manifests/rbac-role-namespace-viewer.yaml , we've created three resources: ServiceAccount Role RoleBinding The Role has full access to all resources (including batch objects such as jobs). Because it is a Role, it can only be applied to a Namespace.","title":"Restricting a user to a namespace"},{"location":"rbac/#retrieve-certificate-credentials","text":"In creating the ServiceAccount, Kubernetes created a secret that contains the CA and token we need for authentication. 1 kubectl describe serviceaccount learn-k8s-user The name of the secret is in the Tokens value. 1 SECRET_NAME=$(kubectl get serviceaccount learn-k8s-user -o \"jsonpath={.secrets[0]['name']}\") Now let's get the token: 1 kubectl get secret $SECRET_NAME -o \"jsonpath={.data.token}\" And CA data: 1 kubectl get secret $SECRET_NAME -o \"jsonpath={.data['ca\\.crt']}\" Now, let's append a user to our Kube config file ( ~/.kube/config ), adding another entry to users : 1 2 3 4 - name: learn-k8s user: client-key-data: $CA token: $TOKEN Now we can test out the access capabilities of our learn-k8s user: 1 kubectl get secrets --as=learn-k8s --namespace=kube-system You should have received an error indicating you are not allowed to list secrets from the kube-system namespace. The --as=learn-k8s is a great option for testing user access. Now let's change our learn-k8s context entry in the Kube config file to use our new learn-k8s-user user. Now if you run: 1 kubectl get secrets --namespace=kube-system You'll get the same error. Note If you're ever running commands that need Cluster level access, switch back to original context setup for your cluster.)","title":"Retrieve certificate credentials"},{"location":"reconciliation-loop-testing/","text":"Testing the reconciliation loop \u00b6 Let's manually delete all of the Pods from the docker Namespace and see what Kubernetes does. First, let's set up a terminal that will watch the status of the Pods in the docker Namespace. 1 watch kubectl get pods --namespace=docker Or if you don't have watch installed: 1 kubectl get pods --namespace=docker --watch In another terminal, delete all Pods from the docker Namespace. 1 kubectl delete --all pods --namespace=docker Kubernetes observed that the desired state was no longer the current state and took action. Wouldn't it be cool to see what Kubernetes did in more detail? Lets do the same thing again but this time, we'll tail the events log. 1 make event-stream Delete the Pods again. 1 kubectl delete --all pods --namespace=docker Probably more information than you need, but it's great that Kubernetes makes what is happening so transparent.","title":"Reconciliation Loop Testing"},{"location":"reconciliation-loop-testing/#testing-the-reconciliation-loop","text":"Let's manually delete all of the Pods from the docker Namespace and see what Kubernetes does. First, let's set up a terminal that will watch the status of the Pods in the docker Namespace. 1 watch kubectl get pods --namespace=docker Or if you don't have watch installed: 1 kubectl get pods --namespace=docker --watch In another terminal, delete all Pods from the docker Namespace. 1 kubectl delete --all pods --namespace=docker Kubernetes observed that the desired state was no longer the current state and took action. Wouldn't it be cool to see what Kubernetes did in more detail? Lets do the same thing again but this time, we'll tail the events log. 1 make event-stream Delete the Pods again. 1 kubectl delete --all pods --namespace=docker Probably more information than you need, but it's great that Kubernetes makes what is happening so transparent.","title":"Testing the reconciliation loop"},{"location":"replicasets/","text":"ReplicaSets Lab \u00b6 This lab is a bit different in that you'll be creating the the replicaset.yaml yourself. Code challenge \u00b6 Using the documentation at https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in template/replicaset.yaml that uses the information from the manifests/pod.yaml file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :). For the solution, see the manifests/replicaset-solution.yaml . We will use the solution with a copy of the manifests/replicaset-solution.yaml renamed to manifests/replicaset.yaml below but use your own replicaset.yam file if you completed the code challenge. To deploy: 1 kubectl apply -f manifests/replicaset.yaml To verify: 1 2 kubectl get replicasets kubectl get pods Notice that the name of the pods is different for those that were created by the ReplicaSet. Notice too that the kuard-service now has multiple endpoints. 1 kubectl describe services kuard-service Replicasets are in that they give us a way to manage multiple Pods, but they have no ability to help us change between one version of Pods to another. That's what deployments are for. Clean up now by deleting your replicaset: 1 kubectl delete -f manifests/replicaset.yaml","title":"ReplicaSets"},{"location":"replicasets/#replicasets-lab","text":"This lab is a bit different in that you'll be creating the the replicaset.yaml yourself.","title":"ReplicaSets Lab"},{"location":"replicasets/#code-challenge","text":"Using the documentation at https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in template/replicaset.yaml that uses the information from the manifests/pod.yaml file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :). For the solution, see the manifests/replicaset-solution.yaml . We will use the solution with a copy of the manifests/replicaset-solution.yaml renamed to manifests/replicaset.yaml below but use your own replicaset.yam file if you completed the code challenge. To deploy: 1 kubectl apply -f manifests/replicaset.yaml To verify: 1 2 kubectl get replicasets kubectl get pods Notice that the name of the pods is different for those that were created by the ReplicaSet. Notice too that the kuard-service now has multiple endpoints. 1 kubectl describe services kuard-service Replicasets are in that they give us a way to manage multiple Pods, but they have no ability to help us change between one version of Pods to another. That's what deployments are for. Clean up now by deleting your replicaset: 1 kubectl delete -f manifests/replicaset.yaml","title":"Code challenge"},{"location":"secrets/","text":"Secrets \u00b6 Secrets can be created from literal values and exposed through environment variables and volume mounts. Creating a Secret from literal values \u00b6 Let's create a secret from a literal value. 1 2 kubectl create secret generic api-token --from-literal=API_TOKEN=ksej3839034u7bk28747op21u7d3u7 kubectl get secrets api-token Notice that api-token is of TYPE Opaque ? That means it's a collection of key-pairs as opposed to a type of key that Kubernetes uses internally, such as ServiceAccount credentials or an ImagePullSecret secret. Also note that the value of the secret has been base64 encoded. 1 kubectl get secret api-token -o yaml Delete the secret: 1 kubectl delete secret api-token Creating Secrets from manifest files \u00b6 Now lets create a secret declaratively, first base64 encoding the value. 1 echo -n 'ksej3839034u7bk28747op21u7d3u7' | base64 Then putting it into our Secret manifest (example at manifests/secret-api-token.yaml ). 1 2 3 4 5 6 7 apiVersion : v1 kind : Secret metadata : name : api - token type : Opaque data : API_TOKEN : a3NlajM4MzkwMzR1N2JrMjg3NDdvcDIxdTdkM3U3 Now let's create this secret: 1 kubectl apply -f manifests/secret-api-token.yaml Secret access in environment variables \u00b6 Secrets, like other configuration data are often supplied via environment variables, using the kuard Pod. 1 kubectl apply -f manifests/pod-api-secret.yaml Now print out the environment vars to see API_KEY : 1 kubectl exec -it kuard-pod-api-secret printenv Let's clean-up all resources we've created so far. 1 make delete-all Secrets access via volumes \u00b6 Applications often use .properties or .env files for configuration. Let's use the file in resources/db.properties and mount it inside a kuard Pod. This Pod will also depend on api-token . First, let's create the secrets: 1 2 kubectl apply -f manifests/secret-db-properties.yaml kubectl apply -f manifests/secret-api-token.yaml Now let's create the kuard Pod. 1 kubectl apply -f manifests/pod-db-secret.yaml Then view the environment vars to confirm that the Secret info has been mapped to the environment variables. 1 kubectl exec kuard-pod-db-secret printenv The kuard Pod has a file browser which while being a disaster for security, is really great for labs. Port forward to the Pod: 1 kubectl port-forward kuard-pod-db-secret 8080:8080 Open http://localhost:8080/fs/ and navigate to the /etc/secrets/ directory where you can view the contents of the db.properties file.","title":"Secrets"},{"location":"secrets/#secrets","text":"Secrets can be created from literal values and exposed through environment variables and volume mounts.","title":"Secrets"},{"location":"secrets/#creating-a-secret-from-literal-values","text":"Let's create a secret from a literal value. 1 2 kubectl create secret generic api-token --from-literal=API_TOKEN=ksej3839034u7bk28747op21u7d3u7 kubectl get secrets api-token Notice that api-token is of TYPE Opaque ? That means it's a collection of key-pairs as opposed to a type of key that Kubernetes uses internally, such as ServiceAccount credentials or an ImagePullSecret secret. Also note that the value of the secret has been base64 encoded. 1 kubectl get secret api-token -o yaml Delete the secret: 1 kubectl delete secret api-token","title":"Creating a Secret from literal values"},{"location":"secrets/#creating-secrets-from-manifest-files","text":"Now lets create a secret declaratively, first base64 encoding the value. 1 echo -n 'ksej3839034u7bk28747op21u7d3u7' | base64 Then putting it into our Secret manifest (example at manifests/secret-api-token.yaml ). 1 2 3 4 5 6 7 apiVersion : v1 kind : Secret metadata : name : api - token type : Opaque data : API_TOKEN : a3NlajM4MzkwMzR1N2JrMjg3NDdvcDIxdTdkM3U3 Now let's create this secret: 1 kubectl apply -f manifests/secret-api-token.yaml","title":"Creating Secrets from manifest files"},{"location":"secrets/#secret-access-in-environment-variables","text":"Secrets, like other configuration data are often supplied via environment variables, using the kuard Pod. 1 kubectl apply -f manifests/pod-api-secret.yaml Now print out the environment vars to see API_KEY : 1 kubectl exec -it kuard-pod-api-secret printenv Let's clean-up all resources we've created so far. 1 make delete-all","title":"Secret access in environment variables"},{"location":"secrets/#secrets-access-via-volumes","text":"Applications often use .properties or .env files for configuration. Let's use the file in resources/db.properties and mount it inside a kuard Pod. This Pod will also depend on api-token . First, let's create the secrets: 1 2 kubectl apply -f manifests/secret-db-properties.yaml kubectl apply -f manifests/secret-api-token.yaml Now let's create the kuard Pod. 1 kubectl apply -f manifests/pod-db-secret.yaml Then view the environment vars to confirm that the Secret info has been mapped to the environment variables. 1 kubectl exec kuard-pod-db-secret printenv The kuard Pod has a file browser which while being a disaster for security, is really great for labs. Port forward to the Pod: 1 kubectl port-forward kuard-pod-db-secret 8080:8080 Open http://localhost:8080/fs/ and navigate to the /etc/secrets/ directory where you can view the contents of the db.properties file.","title":"Secrets access via volumes"},{"location":"services/","text":"Services Lab \u00b6 To illustrate the dynamic nature of Kubernetes Services and Pods, we're going to expose a service, but first exposing the Service without any Pods. Let's make sure we don't have any Podsby running: 1 kubectl delete pods --all --namespace=learn-k8s Note This only applies if you're using Docker for Desktop. Because Docker for Desktop does not bind the NodePort associated with the LoadBalancer Service type to localhost (on the host), I will be using the a type of NodePort service. Currently when using type LoadBalancer , every port value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports. Create the Service: 1 kubectl apply -f manifests/service.yaml Inspect the service so we can get the randomly assigned node ports. Note that each port for our service has been assigned a node port: 1 kubectl describe service kuard-service | grep Port A Service defines the interface to which requests are made. Regardless of the ports that the containers in a Pod may expose, it is the ports mapped at the Service level that are used. Confirm we don't have any Pods matching the label the service is querying for: 1 kubectl get pods --selector=app=kuard We can determine the Service doesn't have associated Pods because there are no endpoints associated with the Service. 1 kubectl describe service kuard-service Let's create the kuard Pod again. 1 kubectl apply -f manifests/pod.yaml Watch to see that the Pod is ready: 1 watch kubectl get pods Once it is, you can now see one endpoint. 1 kubectl describe service kuard-service Service name by DNS \u00b6 This must be done through a container running in the same namespace as the service. Now that we've deployed the kuard Pod, we can get the fully qualified domain name (FQDN). 1 2 3 make debug-container nslookup kuard-service nslookup kuard-service.learn-k8s Note The output from nslookup which says \"nslookup: can't resolve '(null)': Name does not resolve\" is expected because there is no DNS server to perform the lookup against. Now let's try making a request to the service. 1 wget kuard-service -q -O - What's neat, is that the Service is constantly monitoring the Pods who's labels match its selector query, and so knows, which Pod IP addresses (endpoints) to route the request to. Exit (to kill) the debug container. The ClusterIP \u00b6 When a Service is created, it is assigned a Cluster IP which is unique and static for the life of the service. The Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info. Let's find the .spec.clusterIP value of the kuard-service . 1 echo $(kubectl get service kuard-service --template=' {{ .spec.clusterIP }} ') Let's verify that this is a legit IP address by hitting it from inside our Kubernetes instance. Docker for Desktop users should run: 1 make docker-vm-shell Now inside the Kubernetes instance, use the IP address you retrieved earlier. 1 wget $SERVICE_IP -q -O - No magic and no hidden IP address schemes. Kubernetes is awesome!","title":"Services"},{"location":"services/#services-lab","text":"To illustrate the dynamic nature of Kubernetes Services and Pods, we're going to expose a service, but first exposing the Service without any Pods. Let's make sure we don't have any Podsby running: 1 kubectl delete pods --all --namespace=learn-k8s Note This only applies if you're using Docker for Desktop. Because Docker for Desktop does not bind the NodePort associated with the LoadBalancer Service type to localhost (on the host), I will be using the a type of NodePort service. Currently when using type LoadBalancer , every port value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports. Create the Service: 1 kubectl apply -f manifests/service.yaml Inspect the service so we can get the randomly assigned node ports. Note that each port for our service has been assigned a node port: 1 kubectl describe service kuard-service | grep Port A Service defines the interface to which requests are made. Regardless of the ports that the containers in a Pod may expose, it is the ports mapped at the Service level that are used. Confirm we don't have any Pods matching the label the service is querying for: 1 kubectl get pods --selector=app=kuard We can determine the Service doesn't have associated Pods because there are no endpoints associated with the Service. 1 kubectl describe service kuard-service Let's create the kuard Pod again. 1 kubectl apply -f manifests/pod.yaml Watch to see that the Pod is ready: 1 watch kubectl get pods Once it is, you can now see one endpoint. 1 kubectl describe service kuard-service","title":"Services Lab"},{"location":"services/#service-name-by-dns","text":"This must be done through a container running in the same namespace as the service. Now that we've deployed the kuard Pod, we can get the fully qualified domain name (FQDN). 1 2 3 make debug-container nslookup kuard-service nslookup kuard-service.learn-k8s Note The output from nslookup which says \"nslookup: can't resolve '(null)': Name does not resolve\" is expected because there is no DNS server to perform the lookup against. Now let's try making a request to the service. 1 wget kuard-service -q -O - What's neat, is that the Service is constantly monitoring the Pods who's labels match its selector query, and so knows, which Pod IP addresses (endpoints) to route the request to. Exit (to kill) the debug container.","title":"Service name by DNS"},{"location":"services/#the-clusterip","text":"When a Service is created, it is assigned a Cluster IP which is unique and static for the life of the service. The Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info. Let's find the .spec.clusterIP value of the kuard-service . 1 echo $(kubectl get service kuard-service --template=' {{ .spec.clusterIP }} ') Let's verify that this is a legit IP address by hitting it from inside our Kubernetes instance. Docker for Desktop users should run: 1 make docker-vm-shell Now inside the Kubernetes instance, use the IP address you retrieved earlier. 1 wget $SERVICE_IP -q -O - No magic and no hidden IP address schemes. Kubernetes is awesome! <!-- TODO - Creating a Service alias to point to external services (e.g. PostgreSQL instance) outside of the cluster. - Ambassador. >","title":"The ClusterIP"},{"location":"setup/","text":"Development Environment Setup \u00b6 Docker for Desktop \u00b6 I recommend using Docker for Desktop for local development on macOS and Windows. macOS Windows 10 Note Docker for Desktop also installs and configures kubectl , CLI that communicates with the Kubernetes cluster. Warning Docker for Desktop on Windows uses Hyper-V which means it is incompatible with other Hypervisors. I recommend enabling Hyper-V just for the course, then evaluate how you will run a local cluster after. If that's not possible, Minikube is probably your best option. Once Docker for Desktop is running, you need to enable Kubernetes by accessing the Preferences/Settings screen, select the Kubernetes tab, check the Enable Kubernetes checkbox, then click Apply . Linux or Linux VM users \u00b6 I recommend using Minikube or this guide using kubeadm by Liz Rice is recommended. Kubernetes Check \u00b6 Execute the following commands to check kubectl can communicate with your cluster. 1 2 kubectl cluster-info kubectl get nodes Lab \u00b6 Run make setup to pull down the required images for the labs. 1 make setup System utilities \u00b6 kubectl \u00b6 This should have been installed by Docker for Desktop or Minikube. See the kubectl installation instructions if it is not installed. kubeval \u00b6 Validates your Kubernetes files. Installation instructions on GitHub . kubetpl \u00b6 Provides simple variable substitution for Kubernetes files. Installation instructions on GitHub . make \u00b6 Windows users can download Make from Sourceforge . Once installed, add the folder containing the Make.exe binary to your %PATH. watch \u00b6 The watch binary allows you to observe the output from running a command every n seconds. While kubernetes has a built-in --watch flag, I don't use it as it doesn't flush the previous output. jq \u00b6 The jq binary allows us to nicely format, search and extract data from JSON on the command line. For a lot of Kubernetes commands, we'll be using the --template flag which uses Go template syntax but jq is in general, a very useful tool, even if all you use it for is to pretty print JSON. Bash completion for kubectl \u00b6 The kubectl CLI is huge and the completion functionality will help save time. For macOS \u00b6 Install bash-completion for homebrew by running: 1 brew install bash-completion Then add this to your ~/.bash_profile: 1 [ -f /usr/local/etc/bash_completion ] &amp;&amp; . /usr/local/etc/bash_completion Then install the completion for kubectl. 1 kubectl completion bash &gt; $(brew --prefix)/etc/bash_completion.d/kubectl","title":"Setup"},{"location":"setup/#development-environment-setup","text":"","title":"Development Environment Setup"},{"location":"setup/#docker-for-desktop","text":"I recommend using Docker for Desktop for local development on macOS and Windows. macOS Windows 10 Note Docker for Desktop also installs and configures kubectl , CLI that communicates with the Kubernetes cluster. Warning Docker for Desktop on Windows uses Hyper-V which means it is incompatible with other Hypervisors. I recommend enabling Hyper-V just for the course, then evaluate how you will run a local cluster after. If that's not possible, Minikube is probably your best option. Once Docker for Desktop is running, you need to enable Kubernetes by accessing the Preferences/Settings screen, select the Kubernetes tab, check the Enable Kubernetes checkbox, then click Apply .","title":"Docker for Desktop"},{"location":"setup/#linux-or-linux-vm-users","text":"I recommend using Minikube or this guide using kubeadm by Liz Rice is recommended.","title":"Linux or Linux VM users"},{"location":"setup/#kubernetes-check","text":"Execute the following commands to check kubectl can communicate with your cluster. 1 2 kubectl cluster-info kubectl get nodes","title":"Kubernetes Check"},{"location":"setup/#lab","text":"Run make setup to pull down the required images for the labs. 1 make setup","title":"Lab"},{"location":"setup/#system-utilities","text":"","title":"System utilities"},{"location":"setup/#kubectl","text":"This should have been installed by Docker for Desktop or Minikube. See the kubectl installation instructions if it is not installed.","title":"kubectl"},{"location":"setup/#kubeval","text":"Validates your Kubernetes files. Installation instructions on GitHub .","title":"kubeval"},{"location":"setup/#kubetpl","text":"Provides simple variable substitution for Kubernetes files. Installation instructions on GitHub .","title":"kubetpl"},{"location":"setup/#make","text":"Windows users can download Make from Sourceforge . Once installed, add the folder containing the Make.exe binary to your %PATH.","title":"make"},{"location":"setup/#watch","text":"The watch binary allows you to observe the output from running a command every n seconds. While kubernetes has a built-in --watch flag, I don't use it as it doesn't flush the previous output.","title":"watch"},{"location":"setup/#jq","text":"The jq binary allows us to nicely format, search and extract data from JSON on the command line. For a lot of Kubernetes commands, we'll be using the --template flag which uses Go template syntax but jq is in general, a very useful tool, even if all you use it for is to pretty print JSON.","title":"jq"},{"location":"setup/#bash-completion-for-kubectl","text":"The kubectl CLI is huge and the completion functionality will help save time.","title":"Bash completion for kubectl"},{"location":"setup/#for-macos","text":"Install bash-completion for homebrew by running: 1 brew install bash-completion Then add this to your ~/.bash_profile: 1 [ -f /usr/local/etc/bash_completion ] &amp;&amp; . /usr/local/etc/bash_completion Then install the completion for kubectl. 1 kubectl completion bash &gt; $(brew --prefix)/etc/bash_completion.d/kubectl","title":"For macOS"}]}