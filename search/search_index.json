{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to the Learn Kubernetes Labs\n\u00b6\n\n\nThese are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-the-learn-kubernetes-labs",
            "text": "These are simple labs which help you understand how to use the most basic Kubernetes objects in order to deploy a simple containerized application.",
            "title": "Welcome to the Learn Kubernetes Labs"
        },
        {
            "location": "/1-setup/",
            "text": "Development Environment Setup\n\u00b6\n\n\nThese instructions are currently for MacOS users with \nhomebrew\n installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the \nWindows Subsystem for Linux\n.\n\n\nDocker for Mac/Widows Edge Edition\n\u00b6\n\n\nAt time of writing (April 2018), the Edge edition is still required for getting the Kubernetes integrated version of Docker for Desktop. More info at https://www.docker.com/kubernetes\n\n\nThe \nlearn-docker\n containers\n\u00b6\n\n\nThese labs depends on images that are built using the \nhttps://github.com/ryan-blunden/learn-docker\n repository.\n\n\nTo build the required images:\n\n\n1\nmake build\n\n\n\n\n\n\nThe watch command\n\u00b6\n\n\nThe watch command allows you to observe the output from running a command every n seconds.\n\n\nTo install it with homebrew, use \nbrew install watch\n.\n\n\nBash completion for kubectl\n\u00b6\n\n\nThe kubectl CLI is huge and the completion functionality will help save time and help you learn.\n\n\nFor this to work, you need \nbash-completion\n installed by homebrew by running:\n\n\n1\nbrew install bash-completion\n\n\n\n\n\n\nThen add this to your ~/.bash_profile:\n\n\n1\n[ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion\n\n\n\n\n\n\nThen install the completion for kubectl.\n\n\n1\nkubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl",
            "title": "Setup"
        },
        {
            "location": "/1-setup/#development-environment-setup",
            "text": "These instructions are currently for MacOS users with  homebrew  installed. Linux users will be able to easily get the same results while Windows 10 users will have to use a Linux VM or the  Windows Subsystem for Linux .",
            "title": "Development Environment Setup"
        },
        {
            "location": "/1-setup/#docker-for-macwidows-edge-edition",
            "text": "At time of writing (April 2018), the Edge edition is still required for getting the Kubernetes integrated version of Docker for Desktop. More info at https://www.docker.com/kubernetes",
            "title": "Docker for Mac/Widows Edge Edition"
        },
        {
            "location": "/1-setup/#the-learn-docker-containers",
            "text": "These labs depends on images that are built using the  https://github.com/ryan-blunden/learn-docker  repository.  To build the required images:  1 make build",
            "title": "The learn-docker containers"
        },
        {
            "location": "/1-setup/#the-watch-command",
            "text": "The watch command allows you to observe the output from running a command every n seconds.  To install it with homebrew, use  brew install watch .",
            "title": "The watch command"
        },
        {
            "location": "/1-setup/#bash-completion-for-kubectl",
            "text": "The kubectl CLI is huge and the completion functionality will help save time and help you learn.  For this to work, you need  bash-completion  installed by homebrew by running:  1 brew install bash-completion   Then add this to your ~/.bash_profile:  1 [ -f /usr/local/etc/bash_completion ] && . /usr/local/etc/bash_completion   Then install the completion for kubectl.  1 kubectl completion bash > $(brew --prefix)/etc/bash_completion.d/kubectl",
            "title": "Bash completion for kubectl"
        },
        {
            "location": "/2-pods/",
            "text": "Pods Lab\n\u00b6\n\n\nIn this lab, we will deploy a pod, the unit of compute to our Kubernetes instance.\n\n\nYou will notice that we \nalways\n use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same.\n\n\nSee the page on \nObject Management using kubetcl\n for more info.\n\n\nDeploy our API pod:\n\n\n1\nkubectl apply -f templates/api-pod.yaml\n\n\n\n\n\n\nObserve that it is running via Docker (just for fun really):\n\n\n1\ndocker container ps --filter name=api-pod\n\n\n\n\n\n\nVerify that the api container is running in the pod and is listening on port 80:\n\n\n1\nkubectl get pods api-pod --template='\n{{\n(\nindex\n \n(\nindex\n \n.spec.containers\n \n0\n)\n.ports\n \n0\n)\n.containerPort\n}}{{\n\"\\n\"\n}}\n'\n\n\n\n\n\n\n\nFor development purposes, often you only need a single pod exposed. If so, then you can just deploy the pod and port forward from your host.\n\n\n1\nkubectl port-forward api-pod 8080:8080\n\n\n\n\n\n\nView the API in your browser at http://localhost:8080/api/v1/users/\n\n\nKill the port-forwarding process in your terminal using a SIGINT (Ctrl+c).\n\n\nLet's inspect our pod (in a few different ways):\n\n\n1\n2\n3\n4\n5\nkubectl get pods api-pod\nkubectl get pods api-pod -o wide\nkubectl describe pods api-pod\nkubectl get pods api-pod -o yaml\nkubectl get pods api-pod -o json\n\n\n\n\n\n\nDocker commands such as \nlogs\n and \nexec\n have pod equivalents.\n\n\nLet's view the logs for our pod:\n\n\n1\nkubectl logs api-pod\n\n\n\n\n\n\nThis works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name.\n\n\n1\nkubectl logs api-pod hermetic-api\n\n\n\n\n\n\nIf we want to tail the logs, we need to supply the \n-f\n flag.\n\n\n1\nkubectl logs api-pod hermetic-api -f\n\n\n\n\n\n\nWouldn't it be great if we could get into a container inside our Pod, as easily as we the \ndocker container exec\n command. Turns out we\n\n\n1\nkubectl exec -it api-pod sh\n\n\n\n\n\n\nBy default, \nkubectl\n will execute the command against the first container in the \nspec.containers\n list. \n\n\nTo specify which pod you want to access, use the \n--container\n (or \n-c\n) option along with the container name:\n\n\n1\nkubectl exec -it api-pod --container hermetic-api sh\n\n\n\n\n\n\nFinally, remove your pod:\n\n\n1\nkubectl delete -f templates/api-pod.yaml\n\n\n\n\n\n\nTODO\n\u00b6\n\n\n\n\nAdd horizontal pod auto-scaling example - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\n\n\nAdd container request and limit for resources - https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/\n\n\nExample of assigning pods to a specific node.\n\n\nAdd QoS with resource constraints - https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/",
            "title": "Pods"
        },
        {
            "location": "/2-pods/#pods-lab",
            "text": "In this lab, we will deploy a pod, the unit of compute to our Kubernetes instance.  You will notice that we  always  use the declarative form for changing the state of our Kubernetes cluster and I strongly encourage you to always do the same.  See the page on  Object Management using kubetcl  for more info.  Deploy our API pod:  1 kubectl apply -f templates/api-pod.yaml   Observe that it is running via Docker (just for fun really):  1 docker container ps --filter name=api-pod   Verify that the api container is running in the pod and is listening on port 80:  1 kubectl get pods api-pod --template=' {{ ( index   ( index   .spec.containers   0 ) .ports   0 ) .containerPort }}{{ \"\\n\" }} '    For development purposes, often you only need a single pod exposed. If so, then you can just deploy the pod and port forward from your host.  1 kubectl port-forward api-pod 8080:8080   View the API in your browser at http://localhost:8080/api/v1/users/  Kill the port-forwarding process in your terminal using a SIGINT (Ctrl+c).  Let's inspect our pod (in a few different ways):  1\n2\n3\n4\n5 kubectl get pods api-pod\nkubectl get pods api-pod -o wide\nkubectl describe pods api-pod\nkubectl get pods api-pod -o yaml\nkubectl get pods api-pod -o json   Docker commands such as  logs  and  exec  have pod equivalents.  Let's view the logs for our pod:  1 kubectl logs api-pod   This works, but beware. If we had more than one container in our Pod, it wouldn't as kubectl would not know which container we want logs for. To do this in a way which will always work, let's use the container name.  1 kubectl logs api-pod hermetic-api   If we want to tail the logs, we need to supply the  -f  flag.  1 kubectl logs api-pod hermetic-api -f   Wouldn't it be great if we could get into a container inside our Pod, as easily as we the  docker container exec  command. Turns out we  1 kubectl exec -it api-pod sh   By default,  kubectl  will execute the command against the first container in the  spec.containers  list.   To specify which pod you want to access, use the  --container  (or  -c ) option along with the container name:  1 kubectl exec -it api-pod --container hermetic-api sh   Finally, remove your pod:  1 kubectl delete -f templates/api-pod.yaml",
            "title": "Pods Lab"
        },
        {
            "location": "/2-pods/#todo",
            "text": "Add horizontal pod auto-scaling example - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/  Add container request and limit for resources - https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/  Example of assigning pods to a specific node.  Add QoS with resource constraints - https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/",
            "title": "TODO"
        },
        {
            "location": "/3-services/",
            "text": "Services Lab\n\u00b6\n\n\nTo illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it.\n\n\n\n\nNote\n\n\nBecause Docker for Desktop does not bind the \nNodePort\n associated with the \nLoadBalancer\n Service type to localhost (on the host), I will be using the \nNodePort\n service template in the below examples.\n\n\nWhen using type \nLoadBalancer\n, every \nport\n value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports.\n\n\n\n\nCreate the service:\n\n\n1\n2\n# `np` is short for NodePort\nkubectl apply -f templates/api-service-node-port.yaml\n\n\n\n\n\n\nInspect the service so we can get the randomly assigned ports:\n\n\n1\nkubectl describe service api-service | grep Port\n\n\n\n\n\n\nConfirm we don't have any pods matching the label the service is querying for:\n\n\n1\nkubectl get pods api-pod\n\n\n\n\n\n\nThen open a new terminal window and create the api pod.\n\n\n1\nkubectl apply -f templates/api-pod.yaml\n\n\n\n\n\n\nService name by DNS\n\u00b6\n\n\nThis must be done through a container running in the same namespace as the service.\n\n\nNow that we've deployed our API pod, we can get the FQDN.\n\n\n1\nkubectl exec -ti <pod-id> nslookup api-service\n\n\n\n\n\n\nThe ClusterIP\n\u00b6\n\n\nWhen a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info.\n\n\nLet's find the \nCluster IP\n value of the \napi-service\n. The Cluster IP is guaranteed not to change.\n\n\n1\necho `kubectl get service api-service --template='\n{{\n.spec.clusterIP\n}}\n'`\n\n\n\n\n\n\n\nTODO\n\u00b6\n\n\n\n\nFile a bug with Docker that the NodePort doesn't work with type LoadBalancer, unless you configure it with type NodePort at some stage.\n\n\nExample with hitting the service from the Docker VM.\n\n\nExample of hitting the service IP address.\n\n\nExample of hitting the service hostname.\n\n\nExample of using the NodePort so you can have services that forward to the same Pod ports.\n\n\nClarify that \"nslookup: can't resolve '(null)': Name does not resolve\" is expected because there is no DNS to perform the lookup against.",
            "title": "Services"
        },
        {
            "location": "/3-services/#services-lab",
            "text": "To illustrate the dynamic and loosely coupled design of Kubernetes, we're going to expose a service, but exposing the service first without any pods to support it.   Note  Because Docker for Desktop does not bind the  NodePort  associated with the  LoadBalancer  Service type to localhost (on the host), I will be using the  NodePort  service template in the below examples.  When using type  LoadBalancer , every  port  value (not the NodePort) in our service is mapped from our host machine to the Docker VM. I have no idea why this is the default as it means that we can't have multiple services which listen on the same ports.   Create the service:  1\n2 # `np` is short for NodePort\nkubectl apply -f templates/api-service-node-port.yaml   Inspect the service so we can get the randomly assigned ports:  1 kubectl describe service api-service | grep Port   Confirm we don't have any pods matching the label the service is querying for:  1 kubectl get pods api-pod   Then open a new terminal window and create the api pod.  1 kubectl apply -f templates/api-pod.yaml",
            "title": "Services Lab"
        },
        {
            "location": "/3-services/#service-name-by-dns",
            "text": "This must be done through a container running in the same namespace as the service.  Now that we've deployed our API pod, we can get the FQDN.  1 kubectl exec -ti <pod-id> nslookup api-service",
            "title": "Service name by DNS"
        },
        {
            "location": "/3-services/#the-clusterip",
            "text": "When a Service is created, it is assigned Cluster IP which is unique for the life of the service. This Service IP is completely virtual though. See https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies for more info.  Let's find the  Cluster IP  value of the  api-service . The Cluster IP is guaranteed not to change.  1 echo `kubectl get service api-service --template=' {{ .spec.clusterIP }} '`",
            "title": "The ClusterIP"
        },
        {
            "location": "/3-services/#todo",
            "text": "File a bug with Docker that the NodePort doesn't work with type LoadBalancer, unless you configure it with type NodePort at some stage.  Example with hitting the service from the Docker VM.  Example of hitting the service IP address.  Example of hitting the service hostname.  Example of using the NodePort so you can have services that forward to the same Pod ports.  Clarify that \"nslookup: can't resolve '(null)': Name does not resolve\" is expected because there is no DNS to perform the lookup against.",
            "title": "TODO"
        },
        {
            "location": "/4-replicas/",
            "text": "Replicas Lab\n\u00b6\n\n\nThis lab is a bit different in that you'll be creating the the \napi-replicaset.yaml\n yourself.\n\n\nCode challenge\n\u00b6\n\n\nUsing the documentation at \nhttps://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\n, create a replicaset resource in \ntemplate/replicaset.yaml\n that uses the information from the \ntemplates/api-pod.yaml\n file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :).\n\n\nFor the solution, see the \ntemplates/api-replicaset-solution.yaml\n.\n\n\nTo deploy:\n\n\n1\nkubectl apply -f templates/api-replicaset.yaml\n\n\n\n\n\n\nTo verify:\n\n\n1\n2\nkubectl get replicasets\nkubectl get pods\n\n\n\n\n\n\nNotice that the name of the pods is different for those that were created by the ReplicaSet.\n\n\n1\n2\n# TODO: Fix. Not working.\n\n\nkubectl get pod api-replicaset-<HASH> --template='\n{{\n(\nindex\n \n(\nindex\n \n.metadata.annotations\n))\n}}{{\n\"\\n\"\n}}\n'\n\n\n\n\n\n\n\nClean up now by deleting your replicaset:\n\n\n1\nkubectl delete -f templates/api-replicaset.yaml",
            "title": "Replicas"
        },
        {
            "location": "/4-replicas/#replicas-lab",
            "text": "This lab is a bit different in that you'll be creating the the  api-replicaset.yaml  yourself.",
            "title": "Replicas Lab"
        },
        {
            "location": "/4-replicas/#code-challenge",
            "text": "Using the documentation at  https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/ , create a replicaset resource in  template/replicaset.yaml  that uses the information from the  templates/api-pod.yaml  file. Set the number of replicas to whatever you like (although less than 20 is probably a good idea :).  For the solution, see the  templates/api-replicaset-solution.yaml .  To deploy:  1 kubectl apply -f templates/api-replicaset.yaml   To verify:  1\n2 kubectl get replicasets\nkubectl get pods   Notice that the name of the pods is different for those that were created by the ReplicaSet.  1\n2 # TODO: Fix. Not working.  kubectl get pod api-replicaset-<HASH> --template=' {{ ( index   ( index   .metadata.annotations )) }}{{ \"\\n\" }} '    Clean up now by deleting your replicaset:  1 kubectl delete -f templates/api-replicaset.yaml",
            "title": "Code challenge"
        },
        {
            "location": "/5-deployments/",
            "text": "Deployments\n\u00b6\n\n\nCreate the deployment object:\n\n\n1\nkubectl apply -f templates/api-deployments.yaml\n\n\n\n\n\n\nGet the state of the deploloyment:\n\n\n1\n2\nkubectl get deployments\nkubectl describe deployment api-deployment\n\n\n\n\n\n\nScale our pods using Deployments\n\u00b6\n\n\nUpdate \nspec.replicas\n in \napi-deployments.yaml\n to be 15, then deploy our changes:\n\n\n1\nkubectl apply -f templates/api-deployments.yaml\n\n\n\n\n\n\nThen observe the state change to the list of pods every second using the \nwatch\n command:\n\n\n1\nwatch -n 1 kubectl get pods\n\n\n\n\n\n\nNotice the old pods don't get deleted straight away. This is because of \nspec.minReadySeconds\n value of 30 seconds.\n\n\nDeploy a new version of our API container\n\u00b6\n\n\nLet's change the data in \napi/data/users/index.json\n to change the data served by our API.\n\n\nThen lets build a new version of the api container:\n\n\n1\nmake api-build VERSION=2.0\n\n\n\n\n\n\nNow update your \napi-deployments.yaml\n file, changing the image tag to \n2.0\n and adding the following under \nspec.template.metadata\n in order to provide a reason for the manifest change. This is something that would be template driven by your CD system.\n\n\n1\n2\nannotations\n:\n\n    \nkubernetes\n.\nio\n/\n \nchange\n-\ncause\n:\n \n'Data changed in API, updating to 2.0'\n\n\n\n\n\n\n\nThen apply your change:\n\n\n1\nkubectl apply -f templates/api-deployments.yaml\n\n\n\n\n\n\nThen observe the state change to the list of pods every second using the \nwatch\n command:\n\n\n1\nwatch -n 1 kubectl get pods\n\n\n\n\n\n\nObserve the deployment history:\n\n\n1\nkubectl rollout history deployment/api-deployment\n\n\n\n\n\n\nDebugging Tip: Isolating a pod or pods from a ReplicaSet\n\u00b6\n\n\nIf a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong.\n\n\nTo do this, you can change or overwrite the labels (depending upon the replicaset label selector).\n\n\nLet's take one of the deployment pods out of service.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n# Get a pod name\nkubectl get pods\n\n# Take it out of service by changing its label\n# NOTE: Changing the label value is only to disassociate it the\nreplicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value.\n\nkubectl label pods api-replicaset-<HASH> --overwrite app=api-quarantined.\n\n\n\n\n\n\nThis will cause Kubernetes to disassociate that pod with the replicaset which in turn, will cause Kubernetes to create a new pod. As this pod is now unmanaged, you can exec into it without fear of it being killed.\n\n\nTODO\n: Add name change as well to make it more obvious which pod was disassociated with the replicaset.",
            "title": "Deployments"
        },
        {
            "location": "/5-deployments/#deployments",
            "text": "Create the deployment object:  1 kubectl apply -f templates/api-deployments.yaml   Get the state of the deploloyment:  1\n2 kubectl get deployments\nkubectl describe deployment api-deployment",
            "title": "Deployments"
        },
        {
            "location": "/5-deployments/#scale-our-pods-using-deployments",
            "text": "Update  spec.replicas  in  api-deployments.yaml  to be 15, then deploy our changes:  1 kubectl apply -f templates/api-deployments.yaml   Then observe the state change to the list of pods every second using the  watch  command:  1 watch -n 1 kubectl get pods   Notice the old pods don't get deleted straight away. This is because of  spec.minReadySeconds  value of 30 seconds.",
            "title": "Scale our pods using Deployments"
        },
        {
            "location": "/5-deployments/#deploy-a-new-version-of-our-api-container",
            "text": "Let's change the data in  api/data/users/index.json  to change the data served by our API.  Then lets build a new version of the api container:  1 make api-build VERSION=2.0   Now update your  api-deployments.yaml  file, changing the image tag to  2.0  and adding the following under  spec.template.metadata  in order to provide a reason for the manifest change. This is something that would be template driven by your CD system.  1\n2 annotations : \n     kubernetes . io /   change - cause :   'Data changed in API, updating to 2.0'    Then apply your change:  1 kubectl apply -f templates/api-deployments.yaml   Then observe the state change to the list of pods every second using the  watch  command:  1 watch -n 1 kubectl get pods   Observe the deployment history:  1 kubectl rollout history deployment/api-deployment",
            "title": "Deploy a new version of our API container"
        },
        {
            "location": "/5-deployments/#debugging-tip-isolating-a-pod-or-pods-from-a-replicaset",
            "text": "If a pod pods begin misbehaving, you may not want it to be killed, as you may want to quarantine it (take it out of service) in order to inspect it figure out what went wrong.  To do this, you can change or overwrite the labels (depending upon the replicaset label selector).  Let's take one of the deployment pods out of service.  1\n2\n3\n4\n5\n6\n7\n8 # Get a pod name\nkubectl get pods\n\n# Take it out of service by changing its label\n# NOTE: Changing the label value is only to disassociate it the\nreplicaset and service. It has nothing to do with the actual health of the pod and Kubernetes does not care about the value.\n\nkubectl label pods api-replicaset-<HASH> --overwrite app=api-quarantined.   This will cause Kubernetes to disassociate that pod with the replicaset which in turn, will cause Kubernetes to create a new pod. As this pod is now unmanaged, you can exec into it without fear of it being killed.  TODO : Add name change as well to make it more obvious which pod was disassociated with the replicaset.",
            "title": "Debugging Tip: Isolating a pod or pods from a ReplicaSet"
        }
    ]
}